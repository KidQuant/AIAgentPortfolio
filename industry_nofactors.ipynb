{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '1926-07-01'\n",
    "END_DATE = '2025-04-30'\n",
    "TRAIN_START_DATE = '1980-01-01'\n",
    "TRAIN_END_DATE = '1999-12-31'\n",
    "TRADE_START_DATE = '2000-01-01'\n",
    "TRADE_END_DATE = END_DATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-04-30'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRADE_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Durbl</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Enrgy</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>HiTec</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Hlth</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Manuf</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    tic  close  open  high   low\n",
       "0 1926-07-01  Durbl  -0.28 -0.28 -0.28 -0.28\n",
       "1 1926-07-01  Enrgy   0.57  0.57  0.57  0.57\n",
       "2 1926-07-01  HiTec  -0.21 -0.21 -0.21 -0.21\n",
       "3 1926-07-01   Hlth   0.97  0.97  0.97  0.97\n",
       "4 1926-07-01  Manuf  -0.23 -0.23 -0.23 -0.23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/Industry_daily.csv')\n",
    "df.head()\n",
    "# Convert the Date column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n",
    "\n",
    "# Create price_long dataframe with proper date formatting\n",
    "price_long = pd.melt(df, id_vars=['Date'], value_vars=['NoDur', 'Durbl', 'Manuf', 'Enrgy', 'HiTec', 'Telcm', 'Shops', 'Hlth', 'Utils', 'Other'])\n",
    "price_long = price_long.rename(columns={'Date': 'date', 'variable': 'tic', 'value': 'close'})\n",
    "\n",
    "# Add required columns with same values as close price since we don't have this data\n",
    "price_long['open'] = price_long['close'] \n",
    "price_long['high'] = price_long['close']\n",
    "price_long['low'] = price_long['close']\n",
    "\n",
    "# Sort by date and tic\n",
    "price_long = price_long.sort_values(['date', 'tic']).reset_index(drop=True)\n",
    "price_long.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Durbl</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Enrgy</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>HiTec</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Hlth</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Manuf</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    tic   close    open    high     low\n",
       "0 1926-07-01  Durbl  0.9972  0.9972  0.9972  0.9972\n",
       "1 1926-07-01  Enrgy  1.0057  1.0057  1.0057  1.0057\n",
       "2 1926-07-01  HiTec  0.9979  0.9979  0.9979  0.9979\n",
       "3 1926-07-01   Hlth  1.0097  1.0097  1.0097  1.0097\n",
       "4 1926-07-01  Manuf  0.9977  0.9977  0.9977  0.9977"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def returns_to_prices_vectorized(returns_df, initial_price=100):\n",
    "    returns_df = returns_df.copy()\n",
    "    returns_df['close'] = returns_df['close'] / 100 +1\n",
    "\n",
    "    returns_df['close'] = returns_df.groupby('tic')['close'].cumprod()\n",
    "\n",
    "    returns_df['open'] = returns_df['close']\n",
    "    returns_df['high'] = returns_df['close']\n",
    "    returns_df['low'] = returns_df['close']\n",
    "\n",
    "    return returns_df\n",
    "\n",
    "price_long = returns_to_prices_vectorized(price_long, initial_price=100)\n",
    "price_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(259820, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe = FeatureEngineer(\n",
    "    use_technical_indicator=True,\n",
    "    tech_indicator_list=INDICATORS,\n",
    "    use_turbulence=False,\n",
    "    user_defined_feature=False\n",
    ")\n",
    "\n",
    "processed = fe.preprocess_data(price_long)\n",
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Durbl</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.017625</td>\n",
       "      <td>0.987445</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Enrgy</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.017625</td>\n",
       "      <td>0.987445</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0057</td>\n",
       "      <td>1.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>HiTec</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.017625</td>\n",
       "      <td>0.987445</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Hlth</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.017625</td>\n",
       "      <td>0.987445</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0097</td>\n",
       "      <td>1.0097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1926-07-01</td>\n",
       "      <td>Manuf</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.017625</td>\n",
       "      <td>0.987445</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    tic   close    open    high     low  macd   boll_ub   boll_lb  \\\n",
       "0 1926-07-01  Durbl  0.9972  0.9972  0.9972  0.9972   0.0  1.017625  0.987445   \n",
       "1 1926-07-01  Enrgy  1.0057  1.0057  1.0057  1.0057   0.0  1.017625  0.987445   \n",
       "2 1926-07-01  HiTec  0.9979  0.9979  0.9979  0.9979   0.0  1.017625  0.987445   \n",
       "3 1926-07-01   Hlth  1.0097  1.0097  1.0097  1.0097   0.0  1.017625  0.987445   \n",
       "4 1926-07-01  Manuf  0.9977  0.9977  0.9977  0.9977   0.0  1.017625  0.987445   \n",
       "\n",
       "   rsi_30     cci_30  dx_30  close_30_sma  close_60_sma  \n",
       "0   100.0  66.666667  100.0        0.9972        0.9972  \n",
       "1   100.0  66.666667  100.0        1.0057        1.0057  \n",
       "2   100.0  66.666667  100.0        0.9979        0.9979  \n",
       "3   100.0  66.666667  100.0        1.0097        1.0097  \n",
       "4   100.0  66.666667  100.0        0.9977        0.9977  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination = pd.DataFrame(combination, columns=['date', 'tic'])\n",
    "# Convert date column in combination to datetime to match processed dataframe\n",
    "combination['date'] = pd.to_datetime(combination['date'])\n",
    "processed_full = combination.merge(processed, on=['date', 'tic'], how='left')\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date', 'tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)\n",
    "processed_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>cov_list</th>\n",
       "      <th>return_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1927-02-07</td>\n",
       "      <td>Durbl</td>\n",
       "      <td>1.273968</td>\n",
       "      <td>1.273968</td>\n",
       "      <td>1.273968</td>\n",
       "      <td>1.273968</td>\n",
       "      <td>0.007033</td>\n",
       "      <td>1.285140</td>\n",
       "      <td>1.195515</td>\n",
       "      <td>57.275451</td>\n",
       "      <td>81.037788</td>\n",
       "      <td>20.549684</td>\n",
       "      <td>1.249356</td>\n",
       "      <td>1.236923</td>\n",
       "      <td>[[0.0001088125735294116, 4.5430735294117386e-0...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1927-02-07</td>\n",
       "      <td>Enrgy</td>\n",
       "      <td>1.038955</td>\n",
       "      <td>1.038955</td>\n",
       "      <td>1.038955</td>\n",
       "      <td>1.038955</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>1.049766</td>\n",
       "      <td>1.000097</td>\n",
       "      <td>59.375907</td>\n",
       "      <td>115.075788</td>\n",
       "      <td>22.931041</td>\n",
       "      <td>1.019560</td>\n",
       "      <td>1.009704</td>\n",
       "      <td>[[0.0001088125735294116, 4.5430735294117386e-0...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1927-02-07</td>\n",
       "      <td>HiTec</td>\n",
       "      <td>1.046291</td>\n",
       "      <td>1.046291</td>\n",
       "      <td>1.046291</td>\n",
       "      <td>1.046291</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>1.061751</td>\n",
       "      <td>1.026052</td>\n",
       "      <td>50.634648</td>\n",
       "      <td>12.071316</td>\n",
       "      <td>1.605900</td>\n",
       "      <td>1.045212</td>\n",
       "      <td>1.049746</td>\n",
       "      <td>[[0.0001088125735294116, 4.5430735294117386e-0...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1927-02-07</td>\n",
       "      <td>Hlth</td>\n",
       "      <td>1.171732</td>\n",
       "      <td>1.171732</td>\n",
       "      <td>1.171732</td>\n",
       "      <td>1.171732</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>1.192399</td>\n",
       "      <td>1.149500</td>\n",
       "      <td>59.145220</td>\n",
       "      <td>42.812584</td>\n",
       "      <td>12.780824</td>\n",
       "      <td>1.163202</td>\n",
       "      <td>1.143800</td>\n",
       "      <td>[[0.0001088125735294116, 4.5430735294117386e-0...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1927-02-07</td>\n",
       "      <td>Manuf</td>\n",
       "      <td>1.138848</td>\n",
       "      <td>1.138848</td>\n",
       "      <td>1.138848</td>\n",
       "      <td>1.138848</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>1.147861</td>\n",
       "      <td>1.109940</td>\n",
       "      <td>56.327096</td>\n",
       "      <td>92.663498</td>\n",
       "      <td>14.698916</td>\n",
       "      <td>1.129944</td>\n",
       "      <td>1.126999</td>\n",
       "      <td>[[0.0001088125735294116, 4.5430735294117386e-0...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    tic     close      open      high       low      macd  \\\n",
       "0 1927-02-07  Durbl  1.273968  1.273968  1.273968  1.273968  0.007033   \n",
       "1 1927-02-07  Enrgy  1.038955  1.038955  1.038955  1.038955  0.007052   \n",
       "2 1927-02-07  HiTec  1.046291  1.046291  1.046291  1.046291 -0.000609   \n",
       "3 1927-02-07   Hlth  1.171732  1.171732  1.171732  1.171732  0.008811   \n",
       "4 1927-02-07  Manuf  1.138848  1.138848  1.138848  1.138848  0.003147   \n",
       "\n",
       "    boll_ub   boll_lb     rsi_30      cci_30      dx_30  close_30_sma  \\\n",
       "0  1.285140  1.195515  57.275451   81.037788  20.549684      1.249356   \n",
       "1  1.049766  1.000097  59.375907  115.075788  22.931041      1.019560   \n",
       "2  1.061751  1.026052  50.634648   12.071316   1.605900      1.045212   \n",
       "3  1.192399  1.149500  59.145220   42.812584  12.780824      1.163202   \n",
       "4  1.147861  1.109940  56.327096   92.663498  14.698916      1.129944   \n",
       "\n",
       "   close_60_sma                                           cov_list  \\\n",
       "0      1.236923  [[0.0001088125735294116, 4.5430735294117386e-0...   \n",
       "1      1.009704  [[0.0001088125735294116, 4.5430735294117386e-0...   \n",
       "2      1.049746  [[0.0001088125735294116, 4.5430735294117386e-0...   \n",
       "3      1.143800  [[0.0001088125735294116, 4.5430735294117386e-0...   \n",
       "4      1.126999  [[0.0001088125735294116, 4.5430735294117386e-0...   \n",
       "\n",
       "                                         return_list  \n",
       "0  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "1  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "2  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "3  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "4  tic          Durbl   Enrgy   HiTec    Hlth   M...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_full = processed_full.sort_values(['date','tic'], ignore_index=True)\n",
    "processed_full.index = processed_full.date.factorize()[0]\n",
    "\n",
    "cov_list = []\n",
    "return_list = []\n",
    "\n",
    "lookback = 180\n",
    "for i in range(lookback, len(processed_full.index.unique())):\n",
    "    data_lookback = processed_full.iloc[i-lookback:i]\n",
    "    price_lookback = data_lookback.pivot_table(index='date', columns='tic', values='close')\n",
    "    return_lookback = price_lookback.pct_change().dropna()\n",
    "    return_list.append(return_lookback)\n",
    "\n",
    "    cov = return_lookback.cov().values\n",
    "    cov_list.append(cov)\n",
    "\n",
    "df_cov = pd.DataFrame({'date': processed_full.date.unique()[lookback:], 'cov_list':cov_list, 'return_list':return_list})\n",
    "processed_full = processed_full.merge(df_cov, on='date')\n",
    "processed_full = processed_full.sort_values(['date', 'tic']).reset_index(drop=True)\n",
    "\n",
    "processed_full.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE, end=TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE, end=TRADE_END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>cov_list</th>\n",
       "      <th>return_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>Durbl</td>\n",
       "      <td>2814.149482</td>\n",
       "      <td>2814.149482</td>\n",
       "      <td>2814.149482</td>\n",
       "      <td>2814.149482</td>\n",
       "      <td>13.039015</td>\n",
       "      <td>2876.685934</td>\n",
       "      <td>2679.606006</td>\n",
       "      <td>51.168569</td>\n",
       "      <td>58.251081</td>\n",
       "      <td>6.887427</td>\n",
       "      <td>2784.200786</td>\n",
       "      <td>2780.794752</td>\n",
       "      <td>[[0.00035327720588235206, 0.000129704044117647...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>Enrgy</td>\n",
       "      <td>2183.212391</td>\n",
       "      <td>2183.212391</td>\n",
       "      <td>2183.212391</td>\n",
       "      <td>2183.212391</td>\n",
       "      <td>-3.160264</td>\n",
       "      <td>2289.385821</td>\n",
       "      <td>2185.224981</td>\n",
       "      <td>47.018183</td>\n",
       "      <td>-175.185313</td>\n",
       "      <td>12.730794</td>\n",
       "      <td>2247.014282</td>\n",
       "      <td>2210.868755</td>\n",
       "      <td>[[0.00035327720588235206, 0.000129704044117647...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>HiTec</td>\n",
       "      <td>6236.045207</td>\n",
       "      <td>6236.045207</td>\n",
       "      <td>6236.045207</td>\n",
       "      <td>6236.045207</td>\n",
       "      <td>270.560737</td>\n",
       "      <td>6288.033089</td>\n",
       "      <td>5254.516332</td>\n",
       "      <td>76.346012</td>\n",
       "      <td>147.239241</td>\n",
       "      <td>65.773579</td>\n",
       "      <td>5577.952545</td>\n",
       "      <td>5038.895612</td>\n",
       "      <td>[[0.00035327720588235206, 0.000129704044117647...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>Hlth</td>\n",
       "      <td>6104.189747</td>\n",
       "      <td>6104.189747</td>\n",
       "      <td>6104.189747</td>\n",
       "      <td>6104.189747</td>\n",
       "      <td>-44.773822</td>\n",
       "      <td>6328.736086</td>\n",
       "      <td>5859.055836</td>\n",
       "      <td>46.976896</td>\n",
       "      <td>-45.283982</td>\n",
       "      <td>9.254366</td>\n",
       "      <td>6241.225916</td>\n",
       "      <td>6295.155865</td>\n",
       "      <td>[[0.00035327720588235206, 0.000129704044117647...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>Manuf</td>\n",
       "      <td>1946.628385</td>\n",
       "      <td>1946.628385</td>\n",
       "      <td>1946.628385</td>\n",
       "      <td>1946.628385</td>\n",
       "      <td>33.952774</td>\n",
       "      <td>2011.543211</td>\n",
       "      <td>1841.772504</td>\n",
       "      <td>59.118589</td>\n",
       "      <td>71.259150</td>\n",
       "      <td>16.341914</td>\n",
       "      <td>1901.487695</td>\n",
       "      <td>1844.671119</td>\n",
       "      <td>[[0.00035327720588235206, 0.000129704044117647...</td>\n",
       "      <td>tic          Durbl   Enrgy   HiTec    Hlth   M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date    tic        close         open         high          low  \\\n",
       "0 2000-01-03  Durbl  2814.149482  2814.149482  2814.149482  2814.149482   \n",
       "0 2000-01-03  Enrgy  2183.212391  2183.212391  2183.212391  2183.212391   \n",
       "0 2000-01-03  HiTec  6236.045207  6236.045207  6236.045207  6236.045207   \n",
       "0 2000-01-03   Hlth  6104.189747  6104.189747  6104.189747  6104.189747   \n",
       "0 2000-01-03  Manuf  1946.628385  1946.628385  1946.628385  1946.628385   \n",
       "\n",
       "         macd      boll_ub      boll_lb     rsi_30      cci_30      dx_30  \\\n",
       "0   13.039015  2876.685934  2679.606006  51.168569   58.251081   6.887427   \n",
       "0   -3.160264  2289.385821  2185.224981  47.018183 -175.185313  12.730794   \n",
       "0  270.560737  6288.033089  5254.516332  76.346012  147.239241  65.773579   \n",
       "0  -44.773822  6328.736086  5859.055836  46.976896  -45.283982   9.254366   \n",
       "0   33.952774  2011.543211  1841.772504  59.118589   71.259150  16.341914   \n",
       "\n",
       "   close_30_sma  close_60_sma  \\\n",
       "0   2784.200786   2780.794752   \n",
       "0   2247.014282   2210.868755   \n",
       "0   5577.952545   5038.895612   \n",
       "0   6241.225916   6295.155865   \n",
       "0   1901.487695   1844.671119   \n",
       "\n",
       "                                            cov_list  \\\n",
       "0  [[0.00035327720588235206, 0.000129704044117647...   \n",
       "0  [[0.00035327720588235206, 0.000129704044117647...   \n",
       "0  [[0.00035327720588235206, 0.000129704044117647...   \n",
       "0  [[0.00035327720588235206, 0.000129704044117647...   \n",
       "0  [[0.00035327720588235206, 0.000129704044117647...   \n",
       "\n",
       "                                         return_list  \n",
       "0  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "0  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "0  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "0  tic          Durbl   Enrgy   HiTec    Hlth   M...  \n",
       "0  tic          Durbl   Enrgy   HiTec    Hlth   M...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 10, State Space: 10\n"
     ]
    }
   ],
   "source": [
    "# stock_dimension = len(train.tic.unique())\n",
    "# state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "# print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = stock_dimension\n",
    "print(f'Stock Dimension: {stock_dimension}, State Space: {state_space}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    'hmax': 100,\n",
    "    'initial_amount': 1000000,\n",
    "    'transaction_cost_pct': 0.005,\n",
    "    'state_space': state_space,\n",
    "    'stock_dim': stock_dimension,\n",
    "    'tech_indicator_list': INDICATORS,\n",
    "    'action_space': stock_dimension,\n",
    "    'reward_scaling': 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockPortfolioEnv(df=train, **env_kwargs)\n",
    "e_trade_gym = StockPortfolioEnv(df=trade, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = {\n",
    "    \"PPO\": {\n",
    "        'total_timesteps': 50000,\n",
    "        'policy': 'MlpPolicy',\n",
    "        'model_kwargs': {\n",
    "            'learning_rate': 0.0003,\n",
    "            'n_steps': 2048,\n",
    "            'batch_size': 64,\n",
    "            'n_epochs': 10,\n",
    "            'gamma': 0.99,\n",
    "            'gae_lambda': 0.95,\n",
    "            'clip_range': 0.2,\n",
    "            'ent_coef': 0.0,\n",
    "            'vf_coef': 0.5,\n",
    "            'max_grad_norm': 0.5,\n",
    "            # tensorboard_log removed - FinRL handles this separately\n",
    "        },\n",
    "    },\n",
    "    'A2C': {\n",
    "        'total_timesteps': 50000,\n",
    "        'policy': 'MlpPolicy',\n",
    "        'model_kwargs': {\n",
    "            'learning_rate': 0.0007,\n",
    "            'n_steps': 5,\n",
    "            'gamma': 0.99,\n",
    "            'gae_lambda': 1.0,\n",
    "            'ent_coef': 0.01,\n",
    "            'vf_coef': 0.25,\n",
    "            'max_grad_norm': 0.5,\n",
    "            # tensorboard_log removed - FinRL handles this separately\n",
    "        },\n",
    "    },\n",
    "    'DDPG': {\n",
    "        'total_timesteps': 50000,\n",
    "        'policy': 'MlpPolicy',\n",
    "        'model_kwargs': {\n",
    "            'learning_rate': 0.001,\n",
    "            'buffer_size': 1000000,\n",
    "            'learning_starts': 100,\n",
    "            'batch_size': 100,\n",
    "            'tau': 0.005,\n",
    "            'gamma': 0.99,\n",
    "            # tensorboard_log removed - FinRL handles this separately\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training PPO model...\n",
      "==================================================\n",
      "{'learning_rate': 0.0003, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 10, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_range': 0.2, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5}\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 328       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 6         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 3730830.2 |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 305          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.225914e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1e+15        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -1.36e-06    |\n",
      "|    reward               | 12070972.0   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.75e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:24711809.006270543\n",
      "Sharpe:  1.2212633382189613\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.63e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 308          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.005859e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.91e+15     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -2.49e-07    |\n",
      "|    reward               | 1973308.5    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.47e+16     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.63e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 313           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9872248e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.79e+16      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -2.39e-07     |\n",
      "|    reward               | 7009425.5     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.47e+16      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:26189167.885417648\n",
      "Sharpe:  1.236706815206026\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.65e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 315          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.916242e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89e+15     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -6.65e-07    |\n",
      "|    reward               | 1129685.8    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.77e+15     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.65e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 317          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.754394e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.99e+16     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -2.88e-07    |\n",
      "|    reward               | 4018984.8    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.86e+16     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.65e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 317           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 45            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.3236955e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.77e+14      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -1.36e-06     |\n",
      "|    reward               | 12497214.0    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.99e+15      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:23287441.695778154\n",
      "Sharpe:  1.1945334339038256\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.61e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 317           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 51            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1595423e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.55e+15      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -4.66e-07     |\n",
      "|    reward               | 2120045.5     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.6e+16       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.61e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 318           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 57            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5110937e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.1e+16       |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -2.81e-07     |\n",
      "|    reward               | 7106050.0     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.97e+16      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:24408572.291194703\n",
      "Sharpe:  1.213501441695477\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.59e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 319          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.363269e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.26e+15     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -4.67e-07    |\n",
      "|    reward               | 1296380.8    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.15e+15     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 5.06e+03       |\n",
      "|    ep_rew_mean          | 3.59e+10       |\n",
      "| time/                   |                |\n",
      "|    fps                  | 319            |\n",
      "|    iterations           | 11             |\n",
      "|    time_elapsed         | 70             |\n",
      "|    total_timesteps      | 22528          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -1.7462298e-09 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -14.2          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 2.47e+16       |\n",
      "|    n_updates            | 100            |\n",
      "|    policy_gradient_loss | -1.09e-07      |\n",
      "|    reward               | 4421388.0      |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 5.2e+16        |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.59e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 320          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.109499e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+15     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -9.68e-07    |\n",
      "|    reward               | 13819525.0   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.18e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:23174159.214948542\n",
      "Sharpe:  1.1953636705795891\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.57e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 320           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 83            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3655746e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.02e+16      |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -1.93e-07     |\n",
      "|    reward               | 2401704.0     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.86e+16      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.57e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 321          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.958121e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.13e+16     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -3.05e-07    |\n",
      "|    reward               | 8273888.5    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.56e+16     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:27172312.911533967\n",
      "Sharpe:  1.2524364380379458\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.61e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 321          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.032657e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.4e+15      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -8.39e-07    |\n",
      "|    reward               | 1332830.1    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.93e+15     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.61e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 321           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 101           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -8.731149e-11 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.62e+16      |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -1.54e-07     |\n",
      "|    reward               | 5334926.0     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.17e+16      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.61e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 321           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 108           |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.4214768e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.6e+15       |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -4.72e-07     |\n",
      "|    reward               | 18308934.0    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.65e+15      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:26380685.286030933\n",
      "Sharpe:  1.2449927947452903\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.63e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 320          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.627509e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+16     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -3.17e-07    |\n",
      "|    reward               | 2559024.8    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.55e+16     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.63e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 321           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 121           |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2095857e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.22e+16      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -4.2e-07      |\n",
      "|    reward               | 8547764.0     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.77e+16      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:24725349.79748142\n",
      "Sharpe:  1.2157828189419366\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.63e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 321           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 127           |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.3341653e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.52e+15      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -6.73e-07     |\n",
      "|    reward               | 1280778.2     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.15e+15      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.63e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 321           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 133           |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -4.947651e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.26e+16      |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -1.57e-07     |\n",
      "|    reward               | 5252855.5     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.29e+16      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.06e+03     |\n",
      "|    ep_rew_mean          | 3.63e+10     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 321          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.789357e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -14.2        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.51e+15     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -1.11e-06    |\n",
      "|    reward               | 20613468.0   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.27e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25220686.70264138\n",
      "Sharpe:  1.2248970121383043\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.63e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 321           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 146           |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7148205e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.42e+16      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -2.47e-07     |\n",
      "|    reward               | 3349191.8     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.03e+16      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.63e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 322           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 152           |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6356733e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.16e+16      |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -6.95e-07     |\n",
      "|    reward               | 8211937.0     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.97e+16      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:23787972.28186003\n",
      "Sharpe:  1.2050249246680005\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.06e+03      |\n",
      "|    ep_rew_mean          | 3.63e+10      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 322           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 158           |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9558155e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -14.2         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.09e+15      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -3.63e-07     |\n",
      "|    reward               | 1252800.2     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.58e+15      |\n",
      "-------------------------------------------\n",
      "PPO training completed and saved!\n",
      "\n",
      "==================================================\n",
      "Training A2C model...\n",
      "==================================================\n",
      "{'learning_rate': 0.0007, 'n_steps': 5, 'gamma': 0.99, 'gae_lambda': 1.0, 'ent_coef': 0.01, 'vf_coef': 0.25, 'max_grad_norm': 0.5}\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 293       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 5.18e+07  |\n",
      "|    reward             | 1295897.6 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 1.79e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 9.05e+07  |\n",
      "|    reward             | 1981167.6 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 4.32e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 298       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 1.17e+08  |\n",
      "|    reward             | 2693775.0 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 7.61e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 299       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 1.69e+08  |\n",
      "|    reward             | 3215536.2 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 1.21e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 299       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 2.11e+08  |\n",
      "|    reward             | 5058522.5 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 2.71e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 298       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -14       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 2.4e+08   |\n",
      "|    reward             | 6331058.5 |\n",
      "|    std                | 0.977     |\n",
      "|    value_loss         | 4.1e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 3.68e+08  |\n",
      "|    reward             | 8446783.0 |\n",
      "|    std                | 0.977     |\n",
      "|    value_loss         | 7.53e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 4.39e+08   |\n",
      "|    reward             | 10687209.0 |\n",
      "|    std                | 0.973      |\n",
      "|    value_loss         | 1.26e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 7.42e+08   |\n",
      "|    reward             | 17008268.0 |\n",
      "|    std                | 0.97       |\n",
      "|    value_loss         | 3.23e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 299        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 9.21e+08   |\n",
      "|    reward             | 22457008.0 |\n",
      "|    std                | 0.965      |\n",
      "|    value_loss         | 5.44e+15   |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:24800441.18255647\n",
      "Sharpe:  1.2013720673850816\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.56e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 289       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 5.45e+07  |\n",
      "|    reward             | 1241538.8 |\n",
      "|    std                | 0.961     |\n",
      "|    value_loss         | 1.52e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.56e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 289       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 7.59e+07  |\n",
      "|    reward             | 2102146.5 |\n",
      "|    std                | 0.957     |\n",
      "|    value_loss         | 4.63e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.56e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 290       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 9.58e+07  |\n",
      "|    reward             | 2504464.0 |\n",
      "|    std                | 0.953     |\n",
      "|    value_loss         | 7.11e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.56e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 290       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 1.81e+08  |\n",
      "|    reward             | 4609873.0 |\n",
      "|    std                | 0.952     |\n",
      "|    value_loss         | 2.29e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.56e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 2.15e+08  |\n",
      "|    reward             | 5488938.0 |\n",
      "|    std                | 0.952     |\n",
      "|    value_loss         | 3.09e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.56e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 2.12e+08  |\n",
      "|    reward             | 6270089.5 |\n",
      "|    std                | 0.949     |\n",
      "|    value_loss         | 4.02e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.56e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.7     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 3.48e+08  |\n",
      "|    reward             | 8271344.5 |\n",
      "|    std                | 0.949     |\n",
      "|    value_loss         | 7.15e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.56e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 292        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 4.29e+08   |\n",
      "|    reward             | 10483115.0 |\n",
      "|    std                | 0.946      |\n",
      "|    value_loss         | 1.17e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.56e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 292        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 7.01e+08   |\n",
      "|    reward             | 16524345.0 |\n",
      "|    std                | 0.943      |\n",
      "|    value_loss         | 2.77e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.56e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 292        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 1.06e+09   |\n",
      "|    reward             | 24210356.0 |\n",
      "|    std                | 0.94       |\n",
      "|    value_loss         | 6.51e+15   |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25169695.677413635\n",
      "Sharpe:  1.2185770993038947\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 292       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 5.13e+07  |\n",
      "|    reward             | 1343795.2 |\n",
      "|    std                | 0.938     |\n",
      "|    value_loss         | 1.94e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 292       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 8.15e+07  |\n",
      "|    reward             | 2161997.2 |\n",
      "|    std                | 0.937     |\n",
      "|    value_loss         | 4.96e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 292       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 1.1e+08   |\n",
      "|    reward             | 2720822.5 |\n",
      "|    std                | 0.93      |\n",
      "|    value_loss         | 7.55e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 292       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 1.73e+08  |\n",
      "|    reward             | 4531240.0 |\n",
      "|    std                | 0.927     |\n",
      "|    value_loss         | 2.13e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 293       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 2e+08     |\n",
      "|    reward             | 5180592.0 |\n",
      "|    std                | 0.922     |\n",
      "|    value_loss         | 2.95e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 293       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 2.54e+08  |\n",
      "|    reward             | 6218500.5 |\n",
      "|    std                | 0.915     |\n",
      "|    value_loss         | 4.2e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 293       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 2.86e+08  |\n",
      "|    reward             | 8218860.0 |\n",
      "|    std                | 0.915     |\n",
      "|    value_loss         | 7.01e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.59e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 293       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.2     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 3.32e+08  |\n",
      "|    reward             | 9922204.0 |\n",
      "|    std                | 0.909     |\n",
      "|    value_loss         | 1.06e+15  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.59e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 5.11e+08   |\n",
      "|    reward             | 15330668.0 |\n",
      "|    std                | 0.904      |\n",
      "|    value_loss         | 2.41e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.59e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -13.1      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 9.68e+08   |\n",
      "|    reward             | 25180136.0 |\n",
      "|    std                | 0.901      |\n",
      "|    value_loss         | 6.63e+15   |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:26160220.510364458\n",
      "Sharpe:  1.2428772766204585\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 293       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13.1     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 5.53e+07  |\n",
      "|    reward             | 1360085.2 |\n",
      "|    std                | 0.897     |\n",
      "|    value_loss         | 2.01e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 293       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 8.55e+07  |\n",
      "|    reward             | 1868354.9 |\n",
      "|    std                | 0.892     |\n",
      "|    value_loss         | 3.51e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -13       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 8.05e+07  |\n",
      "|    reward             | 2344426.8 |\n",
      "|    std                | 0.886     |\n",
      "|    value_loss         | 5.79e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 1.27e+08  |\n",
      "|    reward             | 4081718.2 |\n",
      "|    std                | 0.88      |\n",
      "|    value_loss         | 1.72e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 1.57e+08  |\n",
      "|    reward             | 4428793.5 |\n",
      "|    std                | 0.879     |\n",
      "|    value_loss         | 2.1e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 2.33e+08  |\n",
      "|    reward             | 5759075.5 |\n",
      "|    std                | 0.879     |\n",
      "|    value_loss         | 3.61e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 2.74e+08  |\n",
      "|    reward             | 7880499.0 |\n",
      "|    std                | 0.878     |\n",
      "|    value_loss         | 6.44e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 3.75e+08  |\n",
      "|    reward             | 8852531.0 |\n",
      "|    std                | 0.876     |\n",
      "|    value_loss         | 8.4e+14   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.64e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 5.14e+08   |\n",
      "|    reward             | 14278931.0 |\n",
      "|    std                | 0.874      |\n",
      "|    value_loss         | 2.15e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.64e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | 7.78e+08   |\n",
      "|    reward             | 21703446.0 |\n",
      "|    std                | 0.871      |\n",
      "|    value_loss         | 5.01e+15   |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25369588.575479623\n",
      "Sharpe:  1.2305613981338108\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | 5.35e+07  |\n",
      "|    reward             | 1266944.0 |\n",
      "|    std                | 0.865     |\n",
      "|    value_loss         | 1.66e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 5.44e+07  |\n",
      "|    reward             | 1711334.8 |\n",
      "|    std                | 0.864     |\n",
      "|    value_loss         | 3.08e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | 9.6e+07   |\n",
      "|    reward             | 2270485.8 |\n",
      "|    std                | 0.859     |\n",
      "|    value_loss         | 5.08e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 1.33e+08  |\n",
      "|    reward             | 3691480.0 |\n",
      "|    std                | 0.853     |\n",
      "|    value_loss         | 1.35e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 1.55e+08  |\n",
      "|    reward             | 4211183.5 |\n",
      "|    std                | 0.852     |\n",
      "|    value_loss         | 1.84e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 2.16e+08  |\n",
      "|    reward             | 5061271.5 |\n",
      "|    std                | 0.852     |\n",
      "|    value_loss         | 2.73e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | 3.01e+08  |\n",
      "|    reward             | 7396782.5 |\n",
      "|    std                | 0.849     |\n",
      "|    value_loss         | 5.69e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | 3.27e+08  |\n",
      "|    reward             | 8211553.0 |\n",
      "|    std                | 0.846     |\n",
      "|    value_loss         | 7.33e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.62e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.5      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | 4.42e+08   |\n",
      "|    reward             | 13466606.0 |\n",
      "|    std                | 0.845      |\n",
      "|    value_loss         | 1.94e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.62e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 7.22e+08   |\n",
      "|    reward             | 21361902.0 |\n",
      "|    std                | 0.838      |\n",
      "|    value_loss         | 4.54e+15   |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25782632.285827518\n",
      "Sharpe:  1.2441553172112563\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.4     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 4.94e+07  |\n",
      "|    reward             | 1304764.9 |\n",
      "|    std                | 0.839     |\n",
      "|    value_loss         | 1.66e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 5.97e+07  |\n",
      "|    reward             | 1610653.8 |\n",
      "|    std                | 0.836     |\n",
      "|    value_loss         | 2.75e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.4     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 8.45e+07  |\n",
      "|    reward             | 2098391.8 |\n",
      "|    std                | 0.836     |\n",
      "|    value_loss         | 4.63e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | 1.15e+08  |\n",
      "|    reward             | 3296468.0 |\n",
      "|    std                | 0.835     |\n",
      "|    value_loss         | 1.14e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 1.51e+08  |\n",
      "|    reward             | 4070578.5 |\n",
      "|    std                | 0.837     |\n",
      "|    value_loss         | 1.76e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.3     |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 1.74e+08  |\n",
      "|    reward             | 4478227.0 |\n",
      "|    std                | 0.833     |\n",
      "|    value_loss         | 2.4e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 2.62e+08  |\n",
      "|    reward             | 6789600.0 |\n",
      "|    std                | 0.832     |\n",
      "|    value_loss         | 5.05e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.61e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.3     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 3.45e+08  |\n",
      "|    reward             | 8405549.0 |\n",
      "|    std                | 0.831     |\n",
      "|    value_loss         | 7.88e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.61e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 4.19e+08   |\n",
      "|    reward             | 12415520.0 |\n",
      "|    std                | 0.83       |\n",
      "|    value_loss         | 1.55e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.61e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 7.34e+08   |\n",
      "|    reward             | 17509238.0 |\n",
      "|    std                | 0.829      |\n",
      "|    value_loss         | 3.33e+15   |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:24361901.778014798\n",
      "Sharpe:  1.2241925696944427\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 4e+07     |\n",
      "|    reward             | 1238704.5 |\n",
      "|    std                | 0.827     |\n",
      "|    value_loss         | 1.59e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 4.51e+07  |\n",
      "|    reward             | 1361669.4 |\n",
      "|    std                | 0.823     |\n",
      "|    value_loss         | 1.62e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 8.53e+07  |\n",
      "|    reward             | 2061374.4 |\n",
      "|    std                | 0.821     |\n",
      "|    value_loss         | 4.47e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 1.17e+08  |\n",
      "|    reward             | 3381322.8 |\n",
      "|    std                | 0.818     |\n",
      "|    value_loss         | 1.16e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 1.57e+08  |\n",
      "|    reward             | 4263127.0 |\n",
      "|    std                | 0.816     |\n",
      "|    value_loss         | 1.86e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 2.14e+08  |\n",
      "|    reward             | 5859387.5 |\n",
      "|    std                | 0.816     |\n",
      "|    value_loss         | 3.83e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 2.71e+08  |\n",
      "|    reward             | 7349959.0 |\n",
      "|    std                | 0.813     |\n",
      "|    value_loss         | 5.55e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.6e+10   |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12.1     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 2.97e+08  |\n",
      "|    reward             | 8683057.0 |\n",
      "|    std                | 0.811     |\n",
      "|    value_loss         | 7.87e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.6e+10    |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 116        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 4.85e+08   |\n",
      "|    reward             | 13060094.0 |\n",
      "|    std                | 0.81       |\n",
      "|    value_loss         | 1.78e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.6e+10    |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -12        |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 6.79e+08   |\n",
      "|    reward             | 21138106.0 |\n",
      "|    std                | 0.807      |\n",
      "|    value_loss         | 4.5e+15    |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:26243638.056454636\n",
      "Sharpe:  1.248272499148534\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 3.63e+07  |\n",
      "|    reward             | 1117140.8 |\n",
      "|    std                | 0.807     |\n",
      "|    value_loss         | 1.26e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -12       |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 5.04e+07  |\n",
      "|    reward             | 1272528.5 |\n",
      "|    std                | 0.803     |\n",
      "|    value_loss         | 1.82e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 7.97e+07  |\n",
      "|    reward             | 1929495.1 |\n",
      "|    std                | 0.8       |\n",
      "|    value_loss         | 4.22e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 1.18e+08  |\n",
      "|    reward             | 3401261.2 |\n",
      "|    std                | 0.8       |\n",
      "|    value_loss         | 1.2e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 1.37e+08  |\n",
      "|    reward             | 3988337.8 |\n",
      "|    std                | 0.795     |\n",
      "|    value_loss         | 1.72e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 1.91e+08  |\n",
      "|    reward             | 5586864.5 |\n",
      "|    std                | 0.793     |\n",
      "|    value_loss         | 3.16e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | 2.28e+08  |\n",
      "|    reward             | 7107857.5 |\n",
      "|    std                | 0.79      |\n",
      "|    value_loss         | 5.43e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.62e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.8     |\n",
      "|    explained_variance | 3.58e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 2.91e+08  |\n",
      "|    reward             | 8540867.0 |\n",
      "|    std                | 0.788     |\n",
      "|    value_loss         | 7.91e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.62e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | 4.22e+08   |\n",
      "|    reward             | 12389045.0 |\n",
      "|    std                | 0.783      |\n",
      "|    value_loss         | 1.73e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.62e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | 7.77e+08   |\n",
      "|    reward             | 21489770.0 |\n",
      "|    std                | 0.78       |\n",
      "|    value_loss         | 4.79e+15   |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25339392.0108683\n",
      "Sharpe:  1.233053775751018\n",
      "=================================\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 5.06e+03 |\n",
      "|    ep_rew_mean        | 3.64e+10 |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 137      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -11.6    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 3.44e+07 |\n",
      "|    reward             | 911111.4 |\n",
      "|    std                | 0.775    |\n",
      "|    value_loss         | 9.96e+12 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 3.78e+07  |\n",
      "|    reward             | 1269802.6 |\n",
      "|    std                | 0.771     |\n",
      "|    value_loss         | 1.62e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 6.19e+07  |\n",
      "|    reward             | 1950924.4 |\n",
      "|    std                | 0.767     |\n",
      "|    value_loss         | 4.17e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 1.16e+08  |\n",
      "|    reward             | 3133546.0 |\n",
      "|    std                | 0.767     |\n",
      "|    value_loss         | 1.03e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 1.47e+08  |\n",
      "|    reward             | 3929960.2 |\n",
      "|    std                | 0.767     |\n",
      "|    value_loss         | 1.61e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 1.79e+08  |\n",
      "|    reward             | 5387915.5 |\n",
      "|    std                | 0.766     |\n",
      "|    value_loss         | 3.03e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.5     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 2.72e+08  |\n",
      "|    reward             | 6906385.0 |\n",
      "|    std                | 0.762     |\n",
      "|    value_loss         | 5.05e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.64e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.4     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 3.22e+08  |\n",
      "|    reward             | 9106860.0 |\n",
      "|    std                | 0.757     |\n",
      "|    value_loss         | 8.67e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.64e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 150        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 3.62e+08   |\n",
      "|    reward             | 11789231.0 |\n",
      "|    std                | 0.754      |\n",
      "|    value_loss         | 1.45e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.64e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 6.3e+08    |\n",
      "|    reward             | 18459914.0 |\n",
      "|    std                | 0.756      |\n",
      "|    value_loss         | 3.7e+15    |\n",
      "--------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:26196829.33178256\n",
      "Sharpe:  1.2492983288302546\n",
      "=================================\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.3     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 9.16e+08  |\n",
      "|    reward             | 1028821.0 |\n",
      "|    std                | 0.753     |\n",
      "|    value_loss         | 7.3e+15   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 4.57e+07  |\n",
      "|    reward             | 1366220.6 |\n",
      "|    std                | 0.75      |\n",
      "|    value_loss         | 2e+13     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 6.16e+07  |\n",
      "|    reward             | 2042354.9 |\n",
      "|    std                | 0.745     |\n",
      "|    value_loss         | 4.54e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 158       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 1.07e+08  |\n",
      "|    reward             | 2984280.0 |\n",
      "|    std                | 0.74      |\n",
      "|    value_loss         | 8.99e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 1.37e+08  |\n",
      "|    reward             | 3461456.0 |\n",
      "|    std                | 0.738     |\n",
      "|    value_loss         | 1.4e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 2.01e+08  |\n",
      "|    reward             | 5816269.0 |\n",
      "|    std                | 0.736     |\n",
      "|    value_loss         | 3.5e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 2.27e+08  |\n",
      "|    reward             | 6719595.0 |\n",
      "|    std                | 0.733     |\n",
      "|    value_loss         | 5.2e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.06e+03  |\n",
      "|    ep_rew_mean        | 3.65e+10  |\n",
      "| time/                 |           |\n",
      "|    fps                | 295       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -11       |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 3.11e+08  |\n",
      "|    reward             | 9298135.0 |\n",
      "|    std                | 0.732     |\n",
      "|    value_loss         | 9.35e+14  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.65e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 296        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 167        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 3.89e+08   |\n",
      "|    reward             | 11848793.0 |\n",
      "|    std                | 0.731      |\n",
      "|    value_loss         | 1.46e+15   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.06e+03   |\n",
      "|    ep_rew_mean        | 3.65e+10   |\n",
      "| time/                 |            |\n",
      "|    fps                | 296        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 168        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -11        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 6.22e+08   |\n",
      "|    reward             | 18980970.0 |\n",
      "|    std                | 0.727      |\n",
      "|    value_loss         | 3.92e+15   |\n",
      "--------------------------------------\n",
      "A2C training completed and saved!\n",
      "\n",
      "==================================================\n",
      "Training DDPG model...\n",
      "==================================================\n",
      "{'learning_rate': 0.001, 'buffer_size': 1000000, 'learning_starts': 100, 'batch_size': 100, 'tau': 0.005, 'gamma': 0.99}\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:26002759.385230828\n",
      "Sharpe:  1.1984289463080096\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 5.06e+03   |\n",
      "|    ep_rew_mean     | 3.56e+10   |\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 138        |\n",
      "|    time_elapsed    | 146        |\n",
      "|    total_timesteps | 20220      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -4.38e+08  |\n",
      "|    critic_loss     | 1.66e+13   |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 20119      |\n",
      "|    reward          | 25963910.0 |\n",
      "-----------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "-----------------------------------\n",
      "| rollout/           |            |\n",
      "|    ep_len_mean     | 5.06e+03   |\n",
      "|    ep_rew_mean     | 3.55e+10   |\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 139        |\n",
      "|    time_elapsed    | 290        |\n",
      "|    total_timesteps | 40440      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -5.99e+08  |\n",
      "|    critic_loss     | 5.19e+13   |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 40339      |\n",
      "|    reward          | 25963910.0 |\n",
      "-----------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:25963909.414110534\n",
      "Sharpe:  1.1962728925554476\n",
      "=================================\n",
      "DDPG training completed and saved!\n",
      "\n",
      "Successfully trained 3 models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in models_to_train.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Create agent\n",
    "        agent = DRLAgent(env=e_train_gym)\n",
    "        \n",
    "        # Get model\n",
    "        model = agent.get_model(\n",
    "            model_name=model_name.lower(),\n",
    "            policy=config['policy'],\n",
    "            model_kwargs=config['model_kwargs']\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        trained_model = agent.train_model(\n",
    "            model=model,\n",
    "            total_timesteps=config['total_timesteps'],\n",
    "            tb_log_name=model_name.lower()\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        model_path = f\"{TRAINED_MODEL_DIR}/{model_name.lower()}_ff_model\"\n",
    "        trained_model.save(model_path)\n",
    "        trained_models[model_name] = trained_model\n",
    "        \n",
    "        print(f\"{model_name} training completed and saved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSuccessfully trained {len(trained_models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models['A2C'].policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TD3Policy(\n",
       "  (actor): Actor(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (mu): Sequential(\n",
       "      (0): Linear(in_features=180, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=400, out_features=300, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=300, out_features=10, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (actor_target): Actor(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (mu): Sequential(\n",
       "      (0): Linear(in_features=180, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=400, out_features=300, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=300, out_features=10, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (critic): ContinuousCritic(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (qf0): Sequential(\n",
       "      (0): Linear(in_features=190, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=400, out_features=300, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=300, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (critic_target): ContinuousCritic(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (qf0): Sequential(\n",
       "      (0): Linear(in_features=190, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=400, out_features=300, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=300, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models['DDPG'].policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContinuousCritic(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (qf0): Sequential(\n",
       "    (0): Linear(in_features=190, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=400, out_features=300, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=300, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models['DDPG'].critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=180, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models['PPO'].policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, model_name, env):\n",
    "    \"\"\"\n",
    "    Test a trained model return results.\n",
    "    \"\"\"\n",
    "    print(f'\\nTesting {model_name}...')\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in range(len(env.get_attr('df')[0].index.unique())-1):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        if dones:\n",
    "            break\n",
    "\n",
    "    # Calculate daily returns from asset_memory\n",
    "    asset_memory = env.get_attr('asset_memory')[0]\n",
    "    daily_returns = np.diff(asset_memory) / asset_memory[:-1]\n",
    "\n",
    "    results = {\n",
    "        'final_value': asset_memory[-1],\n",
    "        'total_return': (asset_memory[-1] / env.get_attr('initial_amount')[0] - 1) * 100,\n",
    "        'daily_returns': daily_returns,\n",
    "        'asset_memory': asset_memory,\n",
    "        'date_memory': env.get_attr('date_memory')[0],\n",
    "        'actions_memory': env.get_attr('actions_memory')[0]\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing PPO...\n",
      "PPO Results:\n",
      "  Final Portfolio Value: $8,133,749.40\n",
      "  Total Return: 713.37%\n",
      "  Sharpe Ratio: 0.5375\n",
      "  Max Drawdown: -0.6260\n",
      "\n",
      "Testing A2C...\n",
      "A2C Results:\n",
      "  Final Portfolio Value: $8,438,735.33\n",
      "  Total Return: 743.87%\n",
      "  Sharpe Ratio: 0.5669\n",
      "  Max Drawdown: -0.6213\n",
      "\n",
      "Testing DDPG...\n",
      "DDPG Results:\n",
      "  Final Portfolio Value: $9,441,400.05\n",
      "  Total Return: 844.14%\n",
      "  Sharpe Ratio: 0.5753\n",
      "  Max Drawdown: -0.5766\n"
     ]
    }
   ],
   "source": [
    "# Test all trained models\n",
    "test_results = {}\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    try:\n",
    "        # Create a vectorized environment for testing\n",
    "        test_env = DummyVecEnv([lambda: e_trade_gym])\n",
    "        \n",
    "        # Reset test environment\n",
    "        obs = test_env.reset()  # This will return just the observation\n",
    "        \n",
    "        # Test model\n",
    "        results = test_model(model, model_name, test_env)\n",
    "        test_results[model_name] = results\n",
    "        \n",
    "        print(f\"{model_name} Results:\")\n",
    "        print(f\"  Final Portfolio Value: ${results['final_value']:,.2f}\")\n",
    "        print(f\"  Total Return: {results['total_return']:.2f}%\")\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        daily_returns = pd.Series(results['daily_returns'])\n",
    "        sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n",
    "        max_drawdown = (daily_returns.cumsum() - daily_returns.cumsum().expanding().max()).min()\n",
    "        \n",
    "        print(f\"  Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
    "        print(f\"  Max Drawdown: {max_drawdown:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {model_name}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Ranking:\n",
      "Model  Final Value  Total Return (%)  Sharpe Ratio  Max Drawdown\n",
      " DDPG 9.441400e+06        844.140005      0.575334     -0.576568\n",
      "  A2C 8.438735e+06        743.873533      0.566905     -0.621298\n",
      "  PPO 8.133749e+06        713.374940      0.537487     -0.625987\n"
     ]
    }
   ],
   "source": [
    "# Create comparsion dataframe\n",
    "comparison_data = []\n",
    "for model_name, results in test_results.items():\n",
    "    daily_returns = pd.Series(results['daily_returns'])\n",
    "    sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n",
    "    max_drawdown = (daily_returns.cumsum() - daily_returns.cumsum().expanding().max()).min()\n",
    "\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Final Value': results['final_value'],\n",
    "        'Total Return (%)': results['total_return'],\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Total Return (%)', ascending=False)\n",
    "\n",
    "print(\"\\nModel Performance Ranking:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "comparison_df.to_csv(f\"{RESULTS_DIR}/industry_model_comparison_{lookback}_window.csv\", index=False)\n",
    "\n",
    "# Create performance visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Portfolio Value Over Time\n",
    "plt.subplot(2, 2, 1)\n",
    "for model_name, results in test_results.items():\n",
    "    dates = pd.to_datetime(results['date_memory'])\n",
    "    values = results['asset_memory']\n",
    "    plt.plot(dates, values, label=f\"{model_name}\", linewidth=2)\n",
    "\n",
    "plt.title('Portfolio Value Over Time (Industry Model: {} Window)'.format(lookback))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 2: Total Returns Comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "models = comparison_df['Model']\n",
    "returns = comparison_df['Total Return (%)']\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "bars = plt.bar(models, returns, color=colors)\n",
    "plt.title('Total Returns Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Total Return (%)')\n",
    "plt.xticks(rotation=45)\n",
    "for bar, ret in zip(bars, returns):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{ret:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Sharpe Ratio Comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "sharpe_ratios = comparison_df['Sharpe Ratio']\n",
    "bars = plt.bar(models, sharpe_ratios, color=colors)\n",
    "plt.title('Sharpe Ratio Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.xticks(rotation=45)\n",
    "for bar, sharpe in zip(bars, sharpe_ratios):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{sharpe:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Max Drawdown Comparison (negative values, so we flip for visualization)\n",
    "plt.subplot(2, 2, 4)\n",
    "drawdowns = comparison_df['Max Drawdown'].abs()\n",
    "bars = plt.bar(models, drawdowns, color=colors)\n",
    "plt.title('Max Drawdown Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Max Drawdown (abs)')\n",
    "plt.xticks(rotation=45)\n",
    "for bar, dd in zip(bars, drawdowns):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{dd:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/industry_model_comparison_{lookback}_window.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nResults saved to {RESULTS_DIR}/\")\n",
    "print(\"Analysis complete!\")\n",
    "\n",
    "# Step 9: Feature Importance Analysis (for the best performing model)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"Analyzing feature importance for best model: {best_model_name}\")\n",
    "\n",
    "# Print FF factor statistics\n",
    "print(\"\\nIndustry Factor Statistics in Dataset:\")\n",
    "for i in ['boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma' ]:\n",
    "    factor_data = processed_full[i]\n",
    "    print(f\"{i}:\")\n",
    "    print(f\"  Mean: {factor_data.mean():.6f}\")\n",
    "    print(f\"  Std:  {factor_data.std():.6f}\")\n",
    "    print(f\"  Min:  {factor_data.min():.6f}\")\n",
    "    print(f\"  Max:  {factor_data.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nTotal Features used: {len()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summerresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
