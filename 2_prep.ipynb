{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b585777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.main import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20b304",
   "metadata": {},
   "source": [
    "### Why The Offline Approach\n",
    "\n",
    "1. **Offline Learning**: Decision Transformers are designed for offline RL - theylearn from existing trajectories rather than interacting with the enviroment during training.\n",
    "2. **Expert Demonstrations**: The PPO model serves as an \"expert\" that provides high-quality trading trajectories. The DT learns to mimic this expert behavior.\n",
    "3. **Conditional Generation**: Unlike PPO which learns a policy directly, the DT learns to generate actions conditioned on:\n",
    "\n",
    "* Current states\n",
    "* Desired returns-to-go (future performance targets)\n",
    "* Timesteps\n",
    "\n",
    "4. **Flexibility**: Once trained, the DT can generate actions for different return targets without retraining, while PPO is fixed to its learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf26a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed63dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>2.724327</td>\n",
       "      <td>2.733033</td>\n",
       "      <td>2.556515</td>\n",
       "      <td>2.578129</td>\n",
       "      <td>746015200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.944414</td>\n",
       "      <td>2.619214</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.724327</td>\n",
       "      <td>2.724327</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>40.463203</td>\n",
       "      <td>40.524938</td>\n",
       "      <td>39.612645</td>\n",
       "      <td>40.188829</td>\n",
       "      <td>6547900</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.944414</td>\n",
       "      <td>2.619214</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.463203</td>\n",
       "      <td>40.463203</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>14.854059</td>\n",
       "      <td>15.000064</td>\n",
       "      <td>14.139404</td>\n",
       "      <td>14.270040</td>\n",
       "      <td>10955700</td>\n",
       "      <td>AXP</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.944414</td>\n",
       "      <td>2.619214</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>14.854059</td>\n",
       "      <td>14.854059</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>33.941093</td>\n",
       "      <td>34.173619</td>\n",
       "      <td>32.088396</td>\n",
       "      <td>32.103398</td>\n",
       "      <td>7010200</td>\n",
       "      <td>BA</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.944414</td>\n",
       "      <td>2.619214</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>33.941093</td>\n",
       "      <td>33.941093</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>30.233912</td>\n",
       "      <td>30.279027</td>\n",
       "      <td>28.815991</td>\n",
       "      <td>28.944894</td>\n",
       "      <td>7117200</td>\n",
       "      <td>CAT</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.944414</td>\n",
       "      <td>2.619214</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>30.233912</td>\n",
       "      <td>30.233912</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      close       high        low       open     volume   tic  \\\n",
       "                                                                             \n",
       "0  2009-01-02   2.724327   2.733033   2.556515   2.578129  746015200  AAPL   \n",
       "0  2009-01-02  40.463203  40.524938  39.612645  40.188829    6547900  AMGN   \n",
       "0  2009-01-02  14.854059  15.000064  14.139404  14.270040   10955700   AXP   \n",
       "0  2009-01-02  33.941093  34.173619  32.088396  32.103398    7010200    BA   \n",
       "0  2009-01-02  30.233912  30.279027  28.815991  28.944894    7117200   CAT   \n",
       "\n",
       "   day  macd   boll_ub   boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
       "                                                                           \n",
       "0    4   0.0  2.944414  2.619214   100.0  66.666667  100.0      2.724327   \n",
       "0    4   0.0  2.944414  2.619214   100.0  66.666667  100.0     40.463203   \n",
       "0    4   0.0  2.944414  2.619214   100.0  66.666667  100.0     14.854059   \n",
       "0    4   0.0  2.944414  2.619214   100.0  66.666667  100.0     33.941093   \n",
       "0    4   0.0  2.944414  2.619214   100.0  66.666667  100.0     30.233912   \n",
       "\n",
       "   close_60_sma        vix  turbulence  \n",
       "                                        \n",
       "0      2.724327  39.189999         0.0  \n",
       "0     40.463203  39.189999         0.0  \n",
       "0     14.854059  39.189999         0.0  \n",
       "0     33.941093  39.189999         0.0  \n",
       "0     30.233912  39.189999         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c82b1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS',\n",
       "        'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM',\n",
       "        'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT'],\n",
       "       dtype=object),\n",
       " ['macd',\n",
       "  'boll_ub',\n",
       "  'boll_lb',\n",
       "  'rsi_30',\n",
       "  'cci_30',\n",
       "  'dx_30',\n",
       "  'close_30_sma',\n",
       "  'close_60_sma'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tic.unique(), INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2d00d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29 State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f'Stock Dimension: {stock_dimension}', f'State Space: {state_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8c1b3",
   "metadata": {},
   "source": [
    "### Stock Universe\n",
    "\n",
    "The model trades 29 stocks from Dow Jones Industrial Average\n",
    "\n",
    "* **Stocks:** AAPL, AMGN, AXP, BA, CAT, CSCO, CVX, DIS, GS, HD, HON, IBM, INTC, JNJ, JPM, KO, MCD, MMM, MRK, MSFT, NKE, PG, TRV, UNH, V, VZ, WBA, WMT\n",
    "\n",
    "### Technical Indicators\n",
    "\n",
    "The environment uses 8 technical indicators for each stock:\n",
    "\n",
    "1. **MACD** - Moving Average Convergence Divergence\n",
    "2. **Bollinger Upper Band**\n",
    "3. **Bollinger Lower Band**\n",
    "4. **RSI (30-period)** - Relative Strength Index\n",
    "5. **CCI (30-period)** - Commodity Channel Index\n",
    "6. **DX (30-period)** - Directional Movement Index\n",
    "7. **Close 30-day SMA** - Simple Moving Average\n",
    "8. **Close 60-day SMA** - Simple Moving Average\n",
    "\n",
    "### State Space Composition\n",
    "\n",
    "The state space has 291 dimensions calculated as follows:\n",
    "\n",
    "$$\\text{State Space}=\\text{Cash Balance}+2\\cdot\\text{Stock Dimensions}+\\text{Indicators}\\cdot\\text{Stock Dimensions}$$\n",
    "\n",
    "### Action Space\n",
    "\n",
    "* 29-dimension continouus action space\n",
    "* Each action represents the number of shares buy/sell for each stock\n",
    "* Actions are bounded by `hmax` (100 shares maximum per trade)\n",
    "\n",
    "### Trading Constraints\n",
    "\n",
    "* **Transaction Costs:** 0.5\\% for both buying and selling (training)\n",
    "* **Position Limits:** Maximum 100 shares per stock per trade\n",
    "* **Initial Capital:** \\$1,000,000\n",
    "* **Reward Scaling:** $e^{-4}$\n",
    "\n",
    "### Data Structure\n",
    "* **Training Period**: Historical data with 3,396 trading days\n",
    "* **Total Data Points**: 98,513 observations ($29\\times 3,396$ days)\n",
    "* **Features**: OHLCV data + technical indicators + VIX + turbulence index\n",
    "\n",
    "This enviroment simulates realistic stock trading with transaction cost, poistion limits, and uses comprehensive technical analysis indicators to inform trading decisions. The model learns to optimize portfolio allocation across the 29 stocks to maximum returns while managing risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d60046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "buy_cost_list = sell_cost_list = [0.005] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    'hmax':100,\n",
    "    'initial_amount': 1000000,\n",
    "    'num_stock_shares': num_stock_shares,\n",
    "    'buy_cost_pct': buy_cost_list,\n",
    "    'sell_cost_pct': sell_cost_list,\n",
    "    'state_space': state_space,\n",
    "    'stock_dim': stock_dimension,\n",
    "    'tech_indicator_list': INDICATORS,\n",
    "    'action_space': stock_dimension,\n",
    "    'reward_scaling': 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f270c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3396"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e_train_gym.df.index.unique()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca4639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(98513)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_train_gym.df.tic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf0001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drebi\\miniconda3\\envs\\summerresearch\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 134        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 15         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.14817615 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02183218 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.0117    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 2.45       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    reward               | 0.06999838 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 7.51       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019586079 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0297     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.97        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    reward               | -0.60969996 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019639552 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0019      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4           |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    reward               | 0.30622664  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 11.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026447022 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.0029      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.63        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    reward               | -0.27280086 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 22          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023148656 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.0188     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    reward               | 0.81511945  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 111        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02625873 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.5      |\n",
      "|    explained_variance   | 0.0118     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 4.05       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    reward               | 3.1857202  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 8.7        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033914126 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.00515     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.68        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | -0.33255267 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024182364 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | -0.0275     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.83        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    reward               | -0.54907507 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 7.82        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027120784 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.0609      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.81        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    reward               | 1.0555567   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 12.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021634005 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.0116      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.31        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | -0.18687826 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 30.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 191        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03794834 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.8      |\n",
      "|    explained_variance   | 0.0349     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.8        |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    reward               | -0.1553412 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 5.67       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034022033 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.19        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    reward               | -0.30922863 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034814853 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | -0.00426    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | 0.5431174   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 23.8        |\n",
      "-----------------------------------------\n",
      "day: 3396, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2050825.04\n",
      "total_reward: 1050825.04\n",
      "total_cost: 1722944.05\n",
      "total_trades: 89086\n",
      "Sharpe: 0.415\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 239        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04062517 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.1      |\n",
      "|    explained_variance   | 0.0129     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.75       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    reward               | 0.22771718 |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 13.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038357876 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.018       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.82        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | -0.84640044 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 23.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 271         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052946795 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0327      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.32        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.00425     |\n",
      "|    reward               | -0.3339162  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 9.42        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029886417 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.057       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 0.25156704  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 24.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 302         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036013767 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | -0.0301     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.05        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    reward               | 0.2221466   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 14.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039162174 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.0966      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.64        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    reward               | -0.22600964 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032057293 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | -0.0647     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00891    |\n",
      "|    reward               | 4.9039435   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037337363 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0376      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.95        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00674    |\n",
      "|    reward               | -2.1392796  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 14.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 366        |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02918513 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.0364     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 43.7       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.00299   |\n",
      "|    reward               | 0.6311421  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 99.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032356765 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0214      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | -1.4148107  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 50.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 128        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 398        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03463196 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.0953     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 9.75       |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    reward               | 0.23074575 |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 24.5       |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "agent = DRLAgent(env = env_train)\n",
    "model_ppo = agent.get_model('ppo')\n",
    "\n",
    "tmp_path = RESULTS_DIR + '/ppo'\n",
    "new_logger_ppo = configure(tmp_path, ['stdout', 'csv', 'tensorboard'])\n",
    "\n",
    "model_ppo.set_logger(new_logger_ppo)\n",
    "\n",
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                                tb_log_name='ppo',\n",
    "                                total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b99e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + '/agent_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db3f3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drebi\\miniconda3\\envs\\summerresearch\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load('trained_models/agent_ppo.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99462d",
   "metadata": {},
   "source": [
    "Here we are training data for the Decision Transformer (DT) model by running pre-trained PPO (Proximal Policy Optimization) reinformcement learning model through a stock trading enviroment.\n",
    "\n",
    "### Main Loop (Lines 16-37)\n",
    "\n",
    "The loop runs through the trading environment step-by-step. This creates offline trajectories with\n",
    "\n",
    "* **States**: Market observations (291-dimensional state spaces)\n",
    "* **Actions**: PPO's trading decisions (29-dimensional action space)\n",
    "* **Rewards**: Trading performance\n",
    "* **Dones**: Episode terminiation flags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7546f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 3396, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7416828.80\n",
      "total_reward: 6416828.80\n",
      "total_cost: 12279.06\n",
      "total_trades: 52303\n",
      "Sharpe: 0.968\n",
      "=================================\n",
      "day: 3396, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7416828.80\n",
      "total_reward: 6416828.80\n",
      "total_cost: 12279.06\n",
      "total_trades: 52303\n",
      "Sharpe: 0.968\n",
      "=================================\n",
      "day: 3396, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7416828.80\n",
      "total_reward: 6416828.80\n",
      "total_cost: 12279.06\n",
      "total_trades: 52303\n",
      "Sharpe: 0.968\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"make a prediction and get results\"\"\"\n",
    "env_train, obs = e_train_gym.get_sb_env()\n",
    "\n",
    "ds = []\n",
    "states = []\n",
    "feature = {}\n",
    "\n",
    "s, a, r, d = [], [], [], []\n",
    "\n",
    "env_train.reset()\n",
    "#max_steps = len(e_train_gym.df.index.unique()) - 1\n",
    "max_steps = e_train_gym.df.tic.count() - 1\n",
    "\n",
    "for i in range(1, max_steps, 1):\n",
    "\n",
    "    action, _states = model.predict(obs, deterministic=true)\n",
    "    s.extend(obs)\n",
    "    a.extend(action)\n",
    "\n",
    "    obs, rewards, dones, info = env_train.step(action)\n",
    "    r.extend(rewards)\n",
    "    d.append(dones[0])\n",
    "\n",
    "    states.extend(obs)\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        \n",
    "        feature['observations'] = s\n",
    "        feature['actions'] = a\n",
    "        feature['rewards'] = r\n",
    "        feature['dones'] = d\n",
    "        \n",
    "        ds.append(feature)\n",
    "        feature = {}\n",
    "        s, a, r, d = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1ffc9",
   "metadata": {},
   "source": [
    "### State Staistics\n",
    "\n",
    "This calculates the mean and standard deviation of all collected states, whcih will be used for normalization in the Decision Transformer training.\n",
    "\n",
    "The purpose is to prepare data for limitation learning. It's collecting expert demonstrations from a trained RL agent (PPO) to train a Deceision Transformer mdoel. The DT will learn to replicate the PPO agent's behavior by observing the state-action-reward sequences.\n",
    "\n",
    "The data structure `ds` contains batches of experiened tuples (observations, actions, rewards, dones) that will be used to train the Decision Transformer to make similiar trading decisions as the PPO model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a886063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.vstack(states)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e3e420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.3963367e+04, 4.1790710e+01, 1.1280511e+02, 7.4285767e+01,\n",
       "        1.5208690e+02], dtype=float32),\n",
       " array([1.6916192e+05, 4.3230621e+01, 5.9250042e+01, 3.7927383e+01,\n",
       "        1.0245756e+02], dtype=float32),\n",
       " (291,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mean[:5], state_std[:5], state_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3bc39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ds = len(ds)\n",
    "\n",
    "state_mean = np.pad(state_mean, (0, (len_ds-state_space)))\n",
    "state_std = np.pad(state_std, (0, (len_ds-state_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8e6e235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.39633672e+04, 4.17907104e+01, 1.12805107e+02, 7.42857666e+01,\n",
       "        1.52086899e+02, 9.07077408e+01, 9.35620880e+01, 2.48966351e+01,\n",
       "        7.07632751e+01, 8.43889084e+01, 1.63063492e+02, 1.14356506e+02,\n",
       "        9.23569031e+01, 9.81931458e+01, 2.87876167e+01, 8.41212234e+01,\n",
       "        6.18488350e+01, 3.15784149e+01, 1.08215553e+02, 8.87638321e+01,\n",
       "        4.15294914e+01, 8.19118958e+01, 5.38253441e+01, 7.01174393e+01,\n",
       "        8.37682190e+01, 1.45445206e+02, 8.84838943e+01, 2.87105122e+01,\n",
       "        3.87024231e+01, 2.41100769e+01, 5.95184278e+00, 1.28901642e+02,\n",
       "        2.36057373e+02, 1.63312183e+03, 3.75818146e+02, 4.07814911e+02,\n",
       "        1.67866621e+04, 9.30251479e-02, 2.70149414e+02, 0.00000000e+00,\n",
       "        2.53899487e+03, 8.81236649e+00, 0.00000000e+00, 3.92849541e+00,\n",
       "        4.09192890e-02, 1.79700432e+01, 1.10966516e+03, 8.56394922e+03,\n",
       "        4.97284470e+01, 9.89319275e+02, 6.81661865e+03, 1.82795227e+02,\n",
       "        8.83150089e-04, 6.32831238e+02, 8.24273471e-03, 5.36839404e+03,\n",
       "        2.65632788e+03, 0.00000000e+00, 6.95494727e+03, 2.83815652e-01,\n",
       "        3.70995015e-01, 2.78782785e-01, 2.14620143e-01, 3.46952647e-01,\n",
       "        3.36825222e-01, 6.17679656e-02, 2.19763920e-01, 1.71129480e-01,\n",
       "        4.46133822e-01, 5.15682995e-01, 3.15836251e-01, 1.49433061e-01,\n",
       "        6.11994937e-02, 2.55986154e-01, 1.92593738e-01, 8.90882611e-02,\n",
       "        3.87806177e-01, 1.54788435e-01, 1.34226099e-01, 4.96994674e-01,\n",
       "        2.02039853e-01, 1.99763417e-01, 2.71563470e-01, 9.09823596e-01,\n",
       "        3.83315742e-01, 5.69484420e-02, 4.00351547e-02, 5.86418621e-02,\n",
       "        4.39843330e+01, 1.17837418e+02, 7.80337143e+01, 1.62202103e+02,\n",
       "        9.58300476e+01, 9.94643860e+01, 2.60779018e+01, 7.39950790e+01,\n",
       "        8.82815323e+01, 1.71859818e+02, 1.19171425e+02, 9.58774643e+01,\n",
       "        1.02150948e+02, 3.03894978e+01, 8.65592804e+01, 6.49529648e+01,\n",
       "        3.25102730e+01, 1.11501633e+02, 9.22167969e+01, 4.30962639e+01,\n",
       "        8.53297729e+01, 5.64787025e+01, 7.21373978e+01, 8.67340088e+01,\n",
       "        1.51446503e+02, 9.20904236e+01, 2.96498852e+01, 4.07030602e+01,\n",
       "        2.49481792e+01, 3.88585014e+01, 1.06761856e+02, 6.98392792e+01,\n",
       "        1.41380722e+02, 8.47145081e+01, 8.67534180e+01, 2.35528755e+01,\n",
       "        6.69731445e+01, 8.00585251e+01, 1.53071136e+02, 1.08194855e+02,\n",
       "        8.80153046e+01, 9.37900848e+01, 2.70329285e+01, 8.09739304e+01,\n",
       "        5.82462921e+01, 3.04033871e+01, 1.03863884e+02, 8.49121780e+01,\n",
       "        3.95906563e+01, 7.71682053e+01, 5.06484261e+01, 6.75637207e+01,\n",
       "        8.00692596e+01, 1.36958878e+02, 8.38570328e+01, 2.76120834e+01,\n",
       "        3.65933266e+01, 2.31205730e+01, 5.56787529e+01, 5.25796509e+01,\n",
       "        5.34438133e+01, 5.30707474e+01, 5.25341492e+01, 5.35147400e+01,\n",
       "        5.25616989e+01, 5.22658424e+01, 5.30131798e+01, 5.19733582e+01,\n",
       "        5.50610847e+01, 5.40022850e+01, 5.17083206e+01, 5.21755447e+01,\n",
       "        5.37307930e+01, 5.29435196e+01, 5.34704704e+01, 5.39139328e+01,\n",
       "        5.30448723e+01, 5.26770477e+01, 5.46670227e+01, 5.36698494e+01,\n",
       "        5.28685799e+01, 5.33540573e+01, 5.47942924e+01, 5.46632309e+01,\n",
       "        5.20768890e+01, 5.15704041e+01, 5.24264030e+01, 3.64594269e+01,\n",
       "        1.75330639e+01, 2.69040623e+01, 2.16401367e+01, 2.26884518e+01,\n",
       "        2.89882927e+01, 1.95140800e+01, 1.77145290e+01, 2.44554291e+01,\n",
       "        1.38375931e+01, 3.38871613e+01, 3.32302704e+01, 1.72301731e+01,\n",
       "        1.93465023e+01, 2.44175568e+01, 2.34931183e+01, 2.83466702e+01,\n",
       "        3.35331306e+01, 2.65219212e+01, 2.21989632e+01, 3.84166145e+01,\n",
       "        2.71224327e+01, 2.28192081e+01, 2.49099483e+01, 3.79439316e+01,\n",
       "        3.96679192e+01, 1.01765833e+01, 7.63044930e+00, 2.12193050e+01,\n",
       "        2.54959087e+01, 2.34476585e+01, 2.22612381e+01, 2.26689510e+01,\n",
       "        2.34138546e+01, 2.33387241e+01, 2.41038952e+01, 2.31459713e+01,\n",
       "        2.44648304e+01, 2.33819771e+01, 2.53659115e+01, 2.20170364e+01,\n",
       "        2.23154602e+01, 2.30067253e+01, 2.31332111e+01, 2.21824512e+01,\n",
       "        2.19383411e+01, 2.27453766e+01, 2.25551510e+01, 2.21823597e+01,\n",
       "        2.32833290e+01, 2.38916168e+01, 2.34949322e+01, 2.28123894e+01,\n",
       "        2.35120640e+01, 2.20379658e+01, 2.24701176e+01, 2.42746258e+01,\n",
       "        2.38330841e+01, 4.12189407e+01, 1.12042725e+02, 7.37377930e+01,\n",
       "        1.51653503e+02, 9.00244827e+01, 9.28667526e+01, 2.47752552e+01,\n",
       "        7.03273773e+01, 8.40544815e+01, 1.62157501e+02, 1.13312912e+02,\n",
       "        9.17214966e+01, 9.78724594e+01, 2.86693611e+01, 8.35948944e+01,\n",
       "        6.14656601e+01, 3.13965473e+01, 1.07416908e+02, 8.84607697e+01,\n",
       "        4.12513771e+01, 8.08976440e+01, 5.34194908e+01, 6.97224197e+01,\n",
       "        8.32119827e+01, 1.43553864e+02, 8.76957703e+01, 2.85924435e+01,\n",
       "        3.86222725e+01, 2.39980431e+01, 4.05853958e+01, 1.11241966e+02,\n",
       "        7.31022110e+01, 1.51176651e+02, 8.92546616e+01, 9.21549759e+01,\n",
       "        2.46343880e+01, 6.98426819e+01, 8.36633530e+01, 1.61169937e+02,\n",
       "        1.12178589e+02, 9.10263443e+01, 9.75620117e+01, 2.85273972e+01,\n",
       "        8.30435715e+01, 6.10397263e+01, 3.11990871e+01, 1.06576859e+02,\n",
       "        8.81104736e+01, 4.09612999e+01, 7.98108826e+01, 5.29748955e+01,\n",
       "        6.92633209e+01, 8.26181183e+01, 1.41582031e+02, 8.68658829e+01,\n",
       "        2.84721508e+01, 3.85328827e+01, 2.38566666e+01, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00], dtype=float32),\n",
       " 985)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mean, len(state_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67422d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds), len(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7893acaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = ds[0]\n",
    "len(feature['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e89032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {}\n",
    "input_data['train'] = ds\n",
    "input_data['state_mean'] = state_mean\n",
    "input_data['state_std'] = state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05ea08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'state_mean', 'state_std'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d56db564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e59ad0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6dc46ca20b4b3ca61e663bac434d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/985 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"data/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83e28154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"data/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377f4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summerresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
