{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b585777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.main import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20b304",
   "metadata": {},
   "source": [
    "### Why The Offline Approach\n",
    "\n",
    "1. **Offline Learning**: Decision Transformers are designed for offline RL - theylearn from existing trajectories rather than interacting with the enviroment during training.\n",
    "2. **Expert Demonstrations**: The PPO model serves as an \"expert\" that provides high-quality trading trajectories. The DT learns to mimic this expert behavior.\n",
    "3. **Conditional Generation**: Unlike PPO which learns a policy directly, the DT learns to generate actions conditioned on:\n",
    "\n",
    "* Current states\n",
    "* Desired returns-to-go (future performance targets)\n",
    "* Timesteps\n",
    "\n",
    "4. **Flexibility**: Once trained, the DT can generate actions for different return targets without retraining, while PPO is fixed to its learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf26a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a6c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/train.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open('data/trade.pickle', 'rb') as f:\n",
    "    trade = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed63dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>cov_list</th>\n",
       "      <th>return_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>19.891676</td>\n",
       "      <td>20.101397</td>\n",
       "      <td>19.783637</td>\n",
       "      <td>19.948872</td>\n",
       "      <td>3815561</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.385154</td>\n",
       "      <td>19.832910</td>\n",
       "      <td>18.381351</td>\n",
       "      <td>66.065031</td>\n",
       "      <td>219.628311</td>\n",
       "      <td>48.080850</td>\n",
       "      <td>18.934376</td>\n",
       "      <td>18.009171</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>4.496878</td>\n",
       "      <td>4.657144</td>\n",
       "      <td>4.393176</td>\n",
       "      <td>4.562870</td>\n",
       "      <td>9837300</td>\n",
       "      <td>AAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264201</td>\n",
       "      <td>4.990097</td>\n",
       "      <td>3.970660</td>\n",
       "      <td>56.734717</td>\n",
       "      <td>56.087390</td>\n",
       "      <td>11.644271</td>\n",
       "      <td>4.087727</td>\n",
       "      <td>3.736084</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>34.948505</td>\n",
       "      <td>35.519729</td>\n",
       "      <td>34.931195</td>\n",
       "      <td>35.225462</td>\n",
       "      <td>1701700</td>\n",
       "      <td>AAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260571</td>\n",
       "      <td>36.124009</td>\n",
       "      <td>34.439074</td>\n",
       "      <td>51.067383</td>\n",
       "      <td>19.990991</td>\n",
       "      <td>0.376685</td>\n",
       "      <td>34.985769</td>\n",
       "      <td>34.059243</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>6.424605</td>\n",
       "      <td>6.439315</td>\n",
       "      <td>6.375673</td>\n",
       "      <td>6.407194</td>\n",
       "      <td>493729600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117870</td>\n",
       "      <td>6.487939</td>\n",
       "      <td>5.524100</td>\n",
       "      <td>62.133201</td>\n",
       "      <td>168.826345</td>\n",
       "      <td>33.760767</td>\n",
       "      <td>6.010477</td>\n",
       "      <td>5.957822</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>18.414785</td>\n",
       "      <td>18.448600</td>\n",
       "      <td>18.232193</td>\n",
       "      <td>18.323490</td>\n",
       "      <td>10829095</td>\n",
       "      <td>ABT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112348</td>\n",
       "      <td>18.481256</td>\n",
       "      <td>17.995301</td>\n",
       "      <td>59.540570</td>\n",
       "      <td>72.995460</td>\n",
       "      <td>13.002825</td>\n",
       "      <td>18.246506</td>\n",
       "      <td>17.839046</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      close       high        low       open     volume   tic  \\\n",
       "0  2010-01-04  19.891676  20.101397  19.783637  19.948872    3815561     A   \n",
       "0  2010-01-04   4.496878   4.657144   4.393176   4.562870    9837300   AAL   \n",
       "0  2010-01-04  34.948505  35.519729  34.931195  35.225462    1701700   AAP   \n",
       "0  2010-01-04   6.424605   6.439315   6.375673   6.407194  493729600  AAPL   \n",
       "0  2010-01-04  18.414785  18.448600  18.232193  18.323490   10829095   ABT   \n",
       "\n",
       "   day      macd    boll_ub    boll_lb     rsi_30      cci_30      dx_30  \\\n",
       "0    0  0.385154  19.832910  18.381351  66.065031  219.628311  48.080850   \n",
       "0    0  0.264201   4.990097   3.970660  56.734717   56.087390  11.644271   \n",
       "0    0  0.260571  36.124009  34.439074  51.067383   19.990991   0.376685   \n",
       "0    0  0.117870   6.487939   5.524100  62.133201  168.826345  33.760767   \n",
       "0    0  0.112348  18.481256  17.995301  59.540570   72.995460  13.002825   \n",
       "\n",
       "   close_30_sma  close_60_sma        vix  turbulence  \\\n",
       "0     18.934376     18.009171  20.040001         0.0   \n",
       "0      4.087727      3.736084  20.040001         0.0   \n",
       "0     34.985769     34.059243  20.040001         0.0   \n",
       "0      6.010477      5.957822  20.040001         0.0   \n",
       "0     18.246506     17.839046  20.040001         0.0   \n",
       "\n",
       "                                            cov_list  \\\n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "\n",
       "                                         return_list  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c82b1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A', 'AAL', 'AAP', 'AAPL', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM',\n",
       "        'ADP', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIV', 'AIZ',\n",
       "        'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL', 'AMAT', 'AMD', 'AME',\n",
       "        'AMG', 'AMGN', 'AMP', 'AMT', 'AMZN', 'AON', 'AOS', 'APA', 'APD',\n",
       "        'APH', 'ARE', 'ATO', 'AVB', 'AVY', 'AWK', 'AXP', 'AZO', 'BA',\n",
       "        'BAC', 'BAX', 'BBT', 'BBY', 'BDX', 'BEN', 'BIIB', 'BK', 'BKNG',\n",
       "        'BLK', 'BMY', 'BR', 'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT',\n",
       "        'CB', 'CBRE', 'CCI', 'CCL', 'CDNS', 'CE', 'CF', 'CHD', 'CHRW',\n",
       "        'CI', 'CINF', 'CL', 'CLX', 'CMA', 'CMCSA', 'CME', 'CMG', 'CMI',\n",
       "        'CMS', 'CNC', 'CNP', 'COF', 'COO', 'COP', 'COST', 'CPB', 'CPRT',\n",
       "        'CRM', 'CSCO', 'CSX', 'CTAS', 'CTSH', 'CVS', 'CVX', 'D', 'DAL',\n",
       "        'DD', 'DE', 'DGX', 'DHI', 'DHR', 'DIS', 'DLR', 'DLTR', 'DOV',\n",
       "        'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'EA', 'EBAY', 'ECL',\n",
       "        'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'EOG', 'EQIX', 'EQR', 'ES',\n",
       "        'ESS', 'ETN', 'ETR', 'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR',\n",
       "        'F', 'FAST', 'FCX', 'FDX', 'FE', 'FFIV', 'FIS', 'FITB', 'FLS',\n",
       "        'FMC', 'FRT', 'FTI', 'GD', 'GE', 'GILD', 'GIS', 'GL', 'GLW',\n",
       "        'GOOG', 'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN',\n",
       "        'HBI', 'HD', 'HIG', 'HOG', 'HOLX', 'HON', 'HP', 'HPQ', 'HRB',\n",
       "        'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IDXX', 'IEX',\n",
       "        'IFF', 'ILMN', 'INCY', 'INTC', 'INTU', 'IP', 'IPG', 'IPGP', 'IRM',\n",
       "        'ISRG', 'IT', 'ITW', 'IVZ', 'JBHT', 'JCI', 'JEF', 'JKHY', 'JNJ',\n",
       "        'JPM', 'K', 'KEY', 'KIM', 'KLAC', 'KMB', 'KMX', 'KO', 'KR', 'KSS',\n",
       "        'L', 'LDOS', 'LEG', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT',\n",
       "        'LNC', 'LNT', 'LOW', 'LRCX', 'LUV', 'M', 'MA', 'MAA', 'MAC', 'MAR',\n",
       "        'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM',\n",
       "        'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS',\n",
       "        'MRK', 'MS', 'MSCI', 'MSFT', 'MSI', 'MTB', 'MTD', 'MU', 'NDAQ',\n",
       "        'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NKTR', 'NOC', 'NOV', 'NRG',\n",
       "        'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWL', 'O', 'OI', 'OKE',\n",
       "        'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PCAR', 'PEG', 'PEP', 'PFE',\n",
       "        'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PLD', 'PM', 'PNC', 'PNR',\n",
       "        'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PVH', 'PWR', 'QCOM',\n",
       "        'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RJF', 'RL', 'RMD', 'ROK',\n",
       "        'ROL', 'ROP', 'ROST', 'RSG', 'SBAC', 'SBUX', 'SCHW', 'SEE', 'SHW',\n",
       "        'SJM', 'SLB', 'SLG', 'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE',\n",
       "        'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYK', 'SYY', 'T', 'TAP',\n",
       "        'TDG', 'TEL', 'TFX', 'TGT', 'TJX', 'TMO', 'TMUS', 'TPR', 'TROW',\n",
       "        'TRV', 'TSCO', 'TSN', 'TTWO', 'TXN', 'TXT', 'UAL', 'UDR', 'UHS',\n",
       "        'ULTA', 'UNH', 'UNM', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VFC',\n",
       "        'VLO', 'VMC', 'VNO', 'VRSN', 'VRTX', 'VTR', 'VZ', 'WAB', 'WAT',\n",
       "        'WBA', 'WDC', 'WEC', 'WELL', 'WFC', 'WHR', 'WM', 'WMB', 'WMT',\n",
       "        'WU', 'WY', 'WYNN', 'XEL', 'XOM', 'XRAY', 'XRX', 'YUM', 'ZBH',\n",
       "        'ZION'], dtype=object),\n",
       " ['macd',\n",
       "  'boll_ub',\n",
       "  'boll_lb',\n",
       "  'rsi_30',\n",
       "  'cci_30',\n",
       "  'dx_30',\n",
       "  'close_30_sma',\n",
       "  'close_60_sma'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tic.unique(), INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2d00d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 394 State Space: 3941\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f'Stock Dimension: {stock_dimension}', f'State Space: {state_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8c1b3",
   "metadata": {},
   "source": [
    "### Stock Universe\n",
    "\n",
    "The model trades 29 stocks from Dow Jones Industrial Average\n",
    "\n",
    "* **Stocks:** AAPL, AMGN, AXP, BA, CAT, CSCO, CVX, DIS, GS, HD, HON, IBM, INTC, JNJ, JPM, KO, MCD, MMM, MRK, MSFT, NKE, PG, TRV, UNH, V, VZ, WBA, WMT\n",
    "\n",
    "### Technical Indicators\n",
    "\n",
    "The environment uses 8 technical indicators for each stock:\n",
    "\n",
    "1. **MACD** - Moving Average Convergence Divergence\n",
    "2. **Bollinger Upper Band**\n",
    "3. **Bollinger Lower Band**\n",
    "4. **RSI (30-period)** - Relative Strength Index\n",
    "5. **CCI (30-period)** - Commodity Channel Index\n",
    "6. **DX (30-period)** - Directional Movement Index\n",
    "7. **Close 30-day SMA** - Simple Moving Average\n",
    "8. **Close 60-day SMA** - Simple Moving Average\n",
    "\n",
    "### State Space Composition\n",
    "\n",
    "The state space has 291 dimensions calculated as follows:\n",
    "\n",
    "$$\\text{State Space}=\\text{Cash Balance}+2\\cdot\\text{Stock Dimensions}+\\text{Indicators}\\cdot\\text{Stock Dimensions}$$\n",
    "\n",
    "### Action Space\n",
    "\n",
    "* 29-dimension continouus action space\n",
    "* Each action represents the number of shares buy/sell for each stock\n",
    "* Actions are bounded by `hmax` (100 shares maximum per trade)\n",
    "\n",
    "### Trading Constraints\n",
    "\n",
    "* **Transaction Costs:** 0.5\\% for both buying and selling (training)\n",
    "* **Position Limits:** Maximum 100 shares per stock per trade\n",
    "* **Initial Capital:** \\$1,000,000\n",
    "* **Reward Scaling:** $e^{-4}$\n",
    "\n",
    "### Data Structure\n",
    "* **Training Period**: Historical data with 3,396 trading days\n",
    "* **Total Data Points**: 98,513 observations ($29\\times 3,396$ days)\n",
    "* **Features**: OHLCV data + technical indicators + VIX + turbulence index\n",
    "\n",
    "This enviroment simulates realistic stock trading with transaction cost, poistion limits, and uses comprehensive technical analysis indicators to inform trading decisions. The model learns to optimize portfolio allocation across the 29 stocks to maximum returns while managing risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d60046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "buy_cost_list = sell_cost_list = [0.005] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    'hmax':100,\n",
    "    'initial_amount': 1000000,\n",
    "    'num_stock_shares': num_stock_shares,\n",
    "    'buy_cost_pct': buy_cost_list,\n",
    "    'sell_cost_pct': sell_cost_list,\n",
    "    'state_space': state_space,\n",
    "    'stock_dim': stock_dimension,\n",
    "    'tech_indicator_list': INDICATORS,\n",
    "    'action_space': stock_dimension,\n",
    "    'reward_scaling': 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f270c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3144"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e_train_gym.df.index.unique()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca4639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1239130)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_train_gym.df.tic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drebi\\miniconda3\\envs\\summerresearch\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 14          |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 140         |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.01169762 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.26951742  |\n",
      "|    clip_fraction        | 0.702       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -561        |\n",
      "|    explained_variance   | -0.0559     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -4.93       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    reward               | -0.06717343 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 421           |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.28732896    |\n",
      "|    clip_fraction        | 0.723         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -564          |\n",
      "|    explained_variance   | 0.493         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -5.31         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0181       |\n",
      "|    reward               | -0.0016762767 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 1.1           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 562           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.15568718    |\n",
      "|    clip_fraction        | 0.673         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -567          |\n",
      "|    explained_variance   | 0.746         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -5.69         |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.02         |\n",
      "|    reward               | -0.0052737696 |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 0.0712        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 703         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.23374766  |\n",
      "|    clip_fraction        | 0.569       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -570        |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.46       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    reward               | -0.27203667 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 1.32        |\n",
      "-----------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 14             |\n",
      "|    iterations           | 6              |\n",
      "|    time_elapsed         | 844            |\n",
      "|    total_timesteps      | 12288          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.3028266      |\n",
      "|    clip_fraction        | 0.683          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -572           |\n",
      "|    explained_variance   | 0.745          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -5.05          |\n",
      "|    n_updates            | 50             |\n",
      "|    policy_gradient_loss | -0.0315        |\n",
      "|    reward               | -0.00089696696 |\n",
      "|    std                  | 1.04           |\n",
      "|    value_loss           | 1.27           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 985          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.19460307   |\n",
      "|    clip_fraction        | 0.664        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -576         |\n",
      "|    explained_variance   | 0.257        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -5.64        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0372      |\n",
      "|    reward               | -0.019357642 |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.381        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1126        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.25840062  |\n",
      "|    clip_fraction        | 0.684       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -579        |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.32       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    reward               | 0.019610202 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 1266         |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.2373248    |\n",
      "|    clip_fraction        | 0.657        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -582         |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -4.92        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    reward               | 0.0046520336 |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 1.39         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 1407         |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.1732398    |\n",
      "|    clip_fraction        | 0.574        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -584         |\n",
      "|    explained_variance   | 0.4          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -5.8         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0163      |\n",
      "|    reward               | -0.047409464 |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 0.419        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 1548       |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.127938   |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -587       |\n",
      "|    explained_variance   | 0.518      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -4.15      |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    reward               | 0.22489354 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 1.81       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 1689       |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20951441 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -589       |\n",
      "|    explained_variance   | 0.639      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -4.79      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0277    |\n",
      "|    reward               | -0.0642244 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 1.26       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 1829       |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10925671 |\n",
      "|    clip_fraction        | 0.548      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -592       |\n",
      "|    explained_variance   | 0.811      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -5.8       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0522    |\n",
      "|    reward               | -0.0922469 |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 0.386      |\n",
      "----------------------------------------\n",
      "day: 3144, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1251.25\n",
      "total_reward: -998748.75\n",
      "total_cost: 1520320.17\n",
      "total_trades: 691970\n",
      "Sharpe: -1.967\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 1970       |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.20497563 |\n",
      "|    clip_fraction        | 0.557      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -594       |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -5.1       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    reward               | 0.17931606 |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 1.6        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 2111        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.18871301  |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -597        |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.79       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    reward               | 0.012698038 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.762       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 2252        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.1386131   |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -601        |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.65       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    reward               | -0.16431648 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 2392       |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21708615 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -604       |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -5.43      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    reward               | 0.35025284 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 1.44       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2533        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.20395109  |\n",
      "|    clip_fraction        | 0.581       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -606        |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.44       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0521     |\n",
      "|    reward               | 0.033899818 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2675        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.20124805  |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -609        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.85       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    reward               | -0.05224251 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 1.13        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 2815       |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19293362 |\n",
      "|    clip_fraction        | 0.551      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -611       |\n",
      "|    explained_variance   | 0.681      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -5.2       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    reward               | 0.7351946  |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 1.89       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 2957        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 30.541483   |\n",
      "|    clip_fraction        | 0.539       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -614        |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.96       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    reward               | 0.013361052 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.365       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 3097        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.16391651  |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -617        |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.73       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    reward               | -0.30344483 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 3237          |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.21268424    |\n",
      "|    clip_fraction        | 0.55          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -619          |\n",
      "|    explained_variance   | 0.703         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -4.77         |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.0577       |\n",
      "|    reward               | -0.0048049353 |\n",
      "|    std                  | 1.17          |\n",
      "|    value_loss           | 1.7           |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 3377        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114578456 |\n",
      "|    clip_fraction        | 0.539       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -622        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -6.17       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    reward               | 0.008544782 |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 0.234       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 3518        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.17805347  |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -625        |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.42       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0497     |\n",
      "|    reward               | 0.052775763 |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = DRLAgent(env = env_train)\n",
    "model_ppo = agent.get_model('ppo')\n",
    "\n",
    "tmp_path = RESULTS_DIR + '/ppo'\n",
    "new_logger_ppo = configure(tmp_path, ['stdout', 'csv', 'tensorboard'])\n",
    "\n",
    "model_ppo.set_logger(new_logger_ppo)\n",
    "\n",
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                                tb_log_name='ppo',\n",
    "                                total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b99e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + '/agent_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db3f3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drebi\\miniconda3\\envs\\summerresearch\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load('trained_models/agent_ppo.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99462d",
   "metadata": {},
   "source": [
    "Here we are training data for the Decision Transformer (DT) model by running pre-trained PPO (Proximal Policy Optimization) reinformcement learning model through a stock trading enviroment.\n",
    "\n",
    "### Main Loop (Lines 16-37)\n",
    "\n",
    "The loop runs through the trading environment step-by-step. This creates offline trajectories with\n",
    "\n",
    "* **States**: Market observations (291-dimensional state spaces)\n",
    "* **Actions**: PPO's trading decisions (29-dimensional action space)\n",
    "* **Rewards**: Trading performance\n",
    "* **Dones**: Episode terminiation flags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b7546f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "day: 3144, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "day: 3144, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "day: 3144, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "day: 3144, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "day: 3144, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "day: 3144, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "day: 3144, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "day: 3144, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "day: 3144, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "day: 3144, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "day: 3144, episode: 150\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "day: 3144, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "day: 3144, episode: 170\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "day: 3144, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "day: 3144, episode: 190\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "day: 3144, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5691979.77\n",
      "total_reward: 4691979.77\n",
      "total_cost: 12681.97\n",
      "total_trades: 600226\n",
      "Sharpe: 0.837\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"make a prediction and get results\"\"\"\n",
    "env_train, obs = e_train_gym.get_sb_env()\n",
    "\n",
    "ds = []\n",
    "states = []\n",
    "feature = {}\n",
    "\n",
    "s, a, r, d = [], [], [], []\n",
    "\n",
    "env_train.reset()\n",
    "# max_steps = len(e_train_gym.df.index.unique()) - 1\n",
    "# max_steps = e_train_gym.df.tic.count() - 1\n",
    "max_steps = 500000\n",
    "\n",
    "for i in range(1, max_steps, 1):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    s.extend(obs)\n",
    "    a.extend(action)\n",
    "\n",
    "    obs, rewards, dones, info = env_train.step(action)\n",
    "    r.extend(rewards)\n",
    "    d.append(dones[0])\n",
    "\n",
    "    states.extend(obs)\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        \n",
    "        feature['observations'] = s\n",
    "        feature['actions'] = a\n",
    "        feature['rewards'] = r\n",
    "        feature['dones'] = d\n",
    "        \n",
    "        ds.append(feature)\n",
    "        feature = {}\n",
    "        s, a, r, d = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1ffc9",
   "metadata": {},
   "source": [
    "### State Staistics\n",
    "\n",
    "This calculates the mean and standard deviation of all collected states, whcih will be used for normalization in the Decision Transformer training.\n",
    "\n",
    "The purpose is to prepare data for limitation learning. It's collecting expert demonstrations from a trained RL agent (PPO) to train a Deceision Transformer mdoel. The DT will learn to replicate the PPO agent's behavior by observing the state-action-reward sequences.\n",
    "\n",
    "The data structure `ds` contains batches of experiened tuples (observations, actions, rewards, dones) that will be used to train the Decision Transformer to make similiar trading decisions as the PPO model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a886063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.vstack(states)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76e3e420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4067.2368  ,   56.01334 ,   25.70047 ,  112.767555,   44.781887],\n",
       "       dtype=float32),\n",
       " array([5.2179863e+04, 3.6167255e+01, 1.4407150e+01, 4.4108833e+01,\n",
       "        4.3545811e+01], dtype=float32),\n",
       " (3941,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mean[:5], state_std[:5], state_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dfa25ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3941"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6272bed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3bc39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ds = len(ds)\n",
    "\n",
    "state_mean = np.pad(state_mean, (0, (len_ds-state_space)))\n",
    "state_std = np.pad(state_std, (0, (len_ds-state_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8e6e235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4067.2368 ,   56.01334,   25.70047, ...,    0.     ,    0.     ,\n",
       "           0.     ], shape=(4999,), dtype=float32),\n",
       " 4999)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mean, len(state_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f67422d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds), len(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7893acaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = ds[0]\n",
    "len(feature['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e89032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {}\n",
    "input_data['train'] = ds\n",
    "input_data['state_mean'] = state_mean\n",
    "input_data['state_std'] = state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ea08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'state_mean', 'state_std'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dfa13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"input_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4c3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"input_data.pkl\", \"rb\") as f:\n",
    "    input_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56db564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59ad0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0373de9c6d344b989628a27c4802b4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/18 shards):   0%|          | 0/4999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"data/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e28154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb2a0cab49449e0af7f30699068ad94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"data/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4a728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summerresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
