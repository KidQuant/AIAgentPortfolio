{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b585777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.main import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20b304",
   "metadata": {},
   "source": [
    "### Why The Offline Approach\n",
    "\n",
    "1. **Offline Learning**: Decision Transformers are designed for offline RL - theylearn from existing trajectories rather than interacting with the enviroment during training.\n",
    "2. **Expert Demonstrations**: The PPO model serves as an \"expert\" that provides high-quality trading trajectories. The DT learns to mimic this expert behavior.\n",
    "3. **Conditional Generation**: Unlike PPO which learns a policy directly, the DT learns to generate actions conditioned on:\n",
    "\n",
    "* Current states\n",
    "* Desired returns-to-go (future performance targets)\n",
    "* Timesteps\n",
    "\n",
    "4. **Flexibility**: Once trained, the DT can generate actions for different return targets without retraining, while PPO is fixed to its learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdf26a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22a6c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/train.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open('data/trade.pickle', 'rb') as f:\n",
    "    trade = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ed63dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "      <th>cov_list</th>\n",
       "      <th>return_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>19.891676</td>\n",
       "      <td>20.101397</td>\n",
       "      <td>19.783637</td>\n",
       "      <td>19.948872</td>\n",
       "      <td>3815561</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.385154</td>\n",
       "      <td>19.832910</td>\n",
       "      <td>18.381351</td>\n",
       "      <td>66.065031</td>\n",
       "      <td>219.628311</td>\n",
       "      <td>48.080850</td>\n",
       "      <td>18.934376</td>\n",
       "      <td>18.009171</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>4.496878</td>\n",
       "      <td>4.657144</td>\n",
       "      <td>4.393176</td>\n",
       "      <td>4.562870</td>\n",
       "      <td>9837300</td>\n",
       "      <td>AAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.264201</td>\n",
       "      <td>4.990097</td>\n",
       "      <td>3.970660</td>\n",
       "      <td>56.734717</td>\n",
       "      <td>56.087390</td>\n",
       "      <td>11.644271</td>\n",
       "      <td>4.087727</td>\n",
       "      <td>3.736084</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>34.948505</td>\n",
       "      <td>35.519729</td>\n",
       "      <td>34.931195</td>\n",
       "      <td>35.225462</td>\n",
       "      <td>1701700</td>\n",
       "      <td>AAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260571</td>\n",
       "      <td>36.124009</td>\n",
       "      <td>34.439074</td>\n",
       "      <td>51.067383</td>\n",
       "      <td>19.990991</td>\n",
       "      <td>0.376685</td>\n",
       "      <td>34.985769</td>\n",
       "      <td>34.059243</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>6.424605</td>\n",
       "      <td>6.439315</td>\n",
       "      <td>6.375673</td>\n",
       "      <td>6.407194</td>\n",
       "      <td>493729600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117870</td>\n",
       "      <td>6.487939</td>\n",
       "      <td>5.524100</td>\n",
       "      <td>62.133201</td>\n",
       "      <td>168.826345</td>\n",
       "      <td>33.760767</td>\n",
       "      <td>6.010477</td>\n",
       "      <td>5.957822</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>18.414785</td>\n",
       "      <td>18.448600</td>\n",
       "      <td>18.232193</td>\n",
       "      <td>18.323490</td>\n",
       "      <td>10829095</td>\n",
       "      <td>ABT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112348</td>\n",
       "      <td>18.481256</td>\n",
       "      <td>17.995301</td>\n",
       "      <td>59.540570</td>\n",
       "      <td>72.995460</td>\n",
       "      <td>13.002825</td>\n",
       "      <td>18.246506</td>\n",
       "      <td>17.839046</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[0.0006651334289736016, 0.0007629606469922543...</td>\n",
       "      <td>tic                A       AAL       AAP      ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      close       high        low       open     volume   tic  \\\n",
       "0  2010-01-04  19.891676  20.101397  19.783637  19.948872    3815561     A   \n",
       "0  2010-01-04   4.496878   4.657144   4.393176   4.562870    9837300   AAL   \n",
       "0  2010-01-04  34.948505  35.519729  34.931195  35.225462    1701700   AAP   \n",
       "0  2010-01-04   6.424605   6.439315   6.375673   6.407194  493729600  AAPL   \n",
       "0  2010-01-04  18.414785  18.448600  18.232193  18.323490   10829095   ABT   \n",
       "\n",
       "   day      macd    boll_ub    boll_lb     rsi_30      cci_30      dx_30  \\\n",
       "0    0  0.385154  19.832910  18.381351  66.065031  219.628311  48.080850   \n",
       "0    0  0.264201   4.990097   3.970660  56.734717   56.087390  11.644271   \n",
       "0    0  0.260571  36.124009  34.439074  51.067383   19.990991   0.376685   \n",
       "0    0  0.117870   6.487939   5.524100  62.133201  168.826345  33.760767   \n",
       "0    0  0.112348  18.481256  17.995301  59.540570   72.995460  13.002825   \n",
       "\n",
       "   close_30_sma  close_60_sma        vix  turbulence  \\\n",
       "0     18.934376     18.009171  20.040001         0.0   \n",
       "0      4.087727      3.736084  20.040001         0.0   \n",
       "0     34.985769     34.059243  20.040001         0.0   \n",
       "0      6.010477      5.957822  20.040001         0.0   \n",
       "0     18.246506     17.839046  20.040001         0.0   \n",
       "\n",
       "                                            cov_list  \\\n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "0  [[0.0006651334289736016, 0.0007629606469922543...   \n",
       "\n",
       "                                         return_list  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  \n",
       "0  tic                A       AAL       AAP      ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c82b1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A', 'AAL', 'AAP', 'AAPL', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM',\n",
       "        'ADP', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIV', 'AIZ',\n",
       "        'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL', 'AMAT', 'AMD', 'AME',\n",
       "        'AMG', 'AMGN', 'AMP', 'AMT', 'AMZN', 'AON', 'AOS', 'APA', 'APD',\n",
       "        'APH', 'ARE', 'ATO', 'AVB', 'AVY', 'AWK', 'AXP', 'AZO', 'BA',\n",
       "        'BAC', 'BAX', 'BBT', 'BBY', 'BDX', 'BEN', 'BIIB', 'BK', 'BKNG',\n",
       "        'BLK', 'BMY', 'BR', 'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT',\n",
       "        'CB', 'CBRE', 'CCI', 'CCL', 'CDNS', 'CE', 'CF', 'CHD', 'CHRW',\n",
       "        'CI', 'CINF', 'CL', 'CLX', 'CMA', 'CMCSA', 'CME', 'CMG', 'CMI',\n",
       "        'CMS', 'CNC', 'CNP', 'COF', 'COO', 'COP', 'COST', 'CPB', 'CPRT',\n",
       "        'CRM', 'CSCO', 'CSX', 'CTAS', 'CTSH', 'CVS', 'CVX', 'D', 'DAL',\n",
       "        'DD', 'DE', 'DGX', 'DHI', 'DHR', 'DIS', 'DLR', 'DLTR', 'DOV',\n",
       "        'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'EA', 'EBAY', 'ECL',\n",
       "        'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'EOG', 'EQIX', 'EQR', 'ES',\n",
       "        'ESS', 'ETN', 'ETR', 'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR',\n",
       "        'F', 'FAST', 'FCX', 'FDX', 'FE', 'FFIV', 'FIS', 'FITB', 'FLS',\n",
       "        'FMC', 'FRT', 'FTI', 'GD', 'GE', 'GILD', 'GIS', 'GL', 'GLW',\n",
       "        'GOOG', 'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN',\n",
       "        'HBI', 'HD', 'HIG', 'HOG', 'HOLX', 'HON', 'HP', 'HPQ', 'HRB',\n",
       "        'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IDXX', 'IEX',\n",
       "        'IFF', 'ILMN', 'INCY', 'INTC', 'INTU', 'IP', 'IPG', 'IPGP', 'IRM',\n",
       "        'ISRG', 'IT', 'ITW', 'IVZ', 'JBHT', 'JCI', 'JEF', 'JKHY', 'JNJ',\n",
       "        'JPM', 'K', 'KEY', 'KIM', 'KLAC', 'KMB', 'KMX', 'KO', 'KR', 'KSS',\n",
       "        'L', 'LDOS', 'LEG', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT',\n",
       "        'LNC', 'LNT', 'LOW', 'LRCX', 'LUV', 'M', 'MA', 'MAA', 'MAC', 'MAR',\n",
       "        'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM',\n",
       "        'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS',\n",
       "        'MRK', 'MS', 'MSCI', 'MSFT', 'MSI', 'MTB', 'MTD', 'MU', 'NDAQ',\n",
       "        'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NKTR', 'NOC', 'NOV', 'NRG',\n",
       "        'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NWL', 'O', 'OI', 'OKE',\n",
       "        'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PCAR', 'PEG', 'PEP', 'PFE',\n",
       "        'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PLD', 'PM', 'PNC', 'PNR',\n",
       "        'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PVH', 'PWR', 'QCOM',\n",
       "        'RCL', 'REG', 'REGN', 'RF', 'RHI', 'RJF', 'RL', 'RMD', 'ROK',\n",
       "        'ROL', 'ROP', 'ROST', 'RSG', 'SBAC', 'SBUX', 'SCHW', 'SEE', 'SHW',\n",
       "        'SJM', 'SLB', 'SLG', 'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE',\n",
       "        'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYK', 'SYY', 'T', 'TAP',\n",
       "        'TDG', 'TEL', 'TFX', 'TGT', 'TJX', 'TMO', 'TMUS', 'TPR', 'TROW',\n",
       "        'TRV', 'TSCO', 'TSN', 'TTWO', 'TXN', 'TXT', 'UAL', 'UDR', 'UHS',\n",
       "        'ULTA', 'UNH', 'UNM', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VFC',\n",
       "        'VLO', 'VMC', 'VNO', 'VRSN', 'VRTX', 'VTR', 'VZ', 'WAB', 'WAT',\n",
       "        'WBA', 'WDC', 'WEC', 'WELL', 'WFC', 'WHR', 'WM', 'WMB', 'WMT',\n",
       "        'WU', 'WY', 'WYNN', 'XEL', 'XOM', 'XRAY', 'XRX', 'YUM', 'ZBH',\n",
       "        'ZION'], dtype=object),\n",
       " ['macd',\n",
       "  'boll_ub',\n",
       "  'boll_lb',\n",
       "  'rsi_30',\n",
       "  'cci_30',\n",
       "  'dx_30',\n",
       "  'close_30_sma',\n",
       "  'close_60_sma'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tic.unique(), INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2d00d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 394 State Space: 3941\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension \n",
    "print(f'Stock Dimension: {stock_dimension}', f'State Space: {state_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8c1b3",
   "metadata": {},
   "source": [
    "### Stock Universe\n",
    "\n",
    "The model trades 29 stocks from Dow Jones Industrial Average\n",
    "\n",
    "* **Stocks:** AAPL, AMGN, AXP, BA, CAT, CSCO, CVX, DIS, GS, HD, HON, IBM, INTC, JNJ, JPM, KO, MCD, MMM, MRK, MSFT, NKE, PG, TRV, UNH, V, VZ, WBA, WMT\n",
    "\n",
    "### Technical Indicators\n",
    "\n",
    "The environment uses 8 technical indicators for each stock:\n",
    "\n",
    "1. **MACD** - Moving Average Convergence Divergence\n",
    "2. **Bollinger Upper Band**\n",
    "3. **Bollinger Lower Band**\n",
    "4. **RSI (30-period)** - Relative Strength Index\n",
    "5. **CCI (30-period)** - Commodity Channel Index\n",
    "6. **DX (30-period)** - Directional Movement Index\n",
    "7. **Close 30-day SMA** - Simple Moving Average\n",
    "8. **Close 60-day SMA** - Simple Moving Average\n",
    "\n",
    "### State Space Composition\n",
    "\n",
    "The state space has 291 dimensions calculated as follows:\n",
    "\n",
    "$$\\text{State Space}=\\text{Cash Balance}+2\\cdot\\text{Stock Dimensions}+\\text{Indicators}\\cdot\\text{Stock Dimensions}$$\n",
    "\n",
    "### Action Space\n",
    "\n",
    "* 29-dimension continouus action space\n",
    "* Each action represents the number of shares buy/sell for each stock\n",
    "* Actions are bounded by `hmax` (100 shares maximum per trade)\n",
    "\n",
    "### Trading Constraints\n",
    "\n",
    "* **Transaction Costs:** 0.5\\% for both buying and selling (training)\n",
    "* **Position Limits:** Maximum 100 shares per stock per trade\n",
    "* **Initial Capital:** \\$1,000,000\n",
    "* **Reward Scaling:** $e^{-4}$\n",
    "\n",
    "### Data Structure\n",
    "* **Training Period**: Historical data with 3,396 trading days\n",
    "* **Total Data Points**: 98,513 observations ($29\\times 3,396$ days)\n",
    "* **Features**: OHLCV data + technical indicators + VIX + turbulence index\n",
    "\n",
    "This enviroment simulates realistic stock trading with transaction cost, poistion limits, and uses comprehensive technical analysis indicators to inform trading decisions. The model learns to optimize portfolio allocation across the 29 stocks to maximum returns while managing risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1d60046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "buy_cost_list = sell_cost_list = [0.005] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    'hmax':100,\n",
    "    'initial_amount': 1000000,\n",
    "    'num_stock_shares': num_stock_shares,\n",
    "    'buy_cost_pct': buy_cost_list,\n",
    "    'sell_cost_pct': sell_cost_list,\n",
    "    'state_space': state_space,\n",
    "    'stock_dim': stock_dimension,\n",
    "    'tech_indicator_list': INDICATORS,\n",
    "    'action_space': stock_dimension,\n",
    "    'reward_scaling': 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df=train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f270c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3144"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e_train_gym.df.index.unique()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ca4639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1239130)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_train_gym.df.tic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdf0001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drebi\\miniconda3\\envs\\summerresearch\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 14           |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 137          |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.020837579 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.26170802  |\n",
      "|    clip_fraction        | 0.693       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -561        |\n",
      "|    explained_variance   | -0.0674     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -4.61       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    reward               | 0.054408308 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.48        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 413           |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.2057108     |\n",
      "|    clip_fraction        | 0.68          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -564          |\n",
      "|    explained_variance   | 0.421         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -5.39         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0399       |\n",
      "|    reward               | -0.0004217968 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 1.34          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 551         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.18775867  |\n",
      "|    clip_fraction        | 0.688       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -567        |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.69       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    reward               | -0.02530686 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 691         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.3611685   |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -570        |\n",
      "|    explained_variance   | 0.587       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.16       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    reward               | -0.36287552 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 829           |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.395276      |\n",
      "|    clip_fraction        | 0.628         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -572          |\n",
      "|    explained_variance   | 0.741         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -5.15         |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.025        |\n",
      "|    reward               | -0.0027879786 |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.32          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 966          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.22288397   |\n",
      "|    clip_fraction        | 0.645        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -575         |\n",
      "|    explained_variance   | 0.774        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -5.67        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0217      |\n",
      "|    reward               | -0.048306663 |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.173        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 1105          |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.53750134    |\n",
      "|    clip_fraction        | 0.671         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -578          |\n",
      "|    explained_variance   | 0.655         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -4.58         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.0206       |\n",
      "|    reward               | -0.0143512245 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.5           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 1243          |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.3792882     |\n",
      "|    clip_fraction        | 0.747         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -581          |\n",
      "|    explained_variance   | 0.656         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -5.38         |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.015        |\n",
      "|    reward               | -0.0018861494 |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 1.87          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 1380          |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.25936282    |\n",
      "|    clip_fraction        | 0.717         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -585          |\n",
      "|    explained_variance   | 0.654         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -5.74         |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | 0.0154        |\n",
      "|    reward               | -0.0058265473 |\n",
      "|    std                  | 1.07          |\n",
      "|    value_loss           | 0.233         |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 1519       |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17521745 |\n",
      "|    clip_fraction        | 0.551      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -588       |\n",
      "|    explained_variance   | 0.627      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -5         |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.049     |\n",
      "|    reward               | 0.28552365 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 1.83       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1663        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.23683709  |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -591        |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -4.99       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    reward               | -0.07135108 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1807        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.10603286  |\n",
      "|    clip_fraction        | 0.551       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -595        |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.86       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    reward               | -0.12332997 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 0.319       |\n",
      "-----------------------------------------\n",
      "day: 3144, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 483.38\n",
      "total_reward: -999516.62\n",
      "total_cost: 1534721.66\n",
      "total_trades: 694361\n",
      "Sharpe: -2.341\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 1944       |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26652423 |\n",
      "|    clip_fraction        | 0.559      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -598       |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -4.67      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    reward               | 0.05148823 |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 2.3        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 2083        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.22539432  |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -601        |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.56       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    reward               | 0.027671799 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 0.971       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 2220        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.15359972  |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -605        |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.87       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    reward               | -0.21431708 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 0.983       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 2359       |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.2517343  |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -608       |\n",
      "|    explained_variance   | 0.578      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -5.57      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    reward               | 0.28685406 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 2.06       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2498        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.32054156  |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -612        |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.8        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    reward               | 0.000266508 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 1.03        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2637        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.43422997  |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -615        |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.71       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    reward               | -0.07002383 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 2774       |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19620286 |\n",
      "|    clip_fraction        | 0.572      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -618       |\n",
      "|    explained_variance   | 0.62       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -5.57      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    reward               | 0.84815484 |\n",
      "|    std                  | 1.17       |\n",
      "|    value_loss           | 1.7        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 2912       |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13836432 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -621       |\n",
      "|    explained_variance   | 0.627      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -6.09      |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0516    |\n",
      "|    reward               | 0.04018342 |\n",
      "|    std                  | 1.18       |\n",
      "|    value_loss           | 0.604      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 3051        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.12711138  |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -624        |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -5.86       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    reward               | -0.16857639 |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 1.46        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 14            |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 3188          |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.11015608    |\n",
      "|    clip_fraction        | 0.526         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -627          |\n",
      "|    explained_variance   | 0.681         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -5.49         |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.0504       |\n",
      "|    reward               | -0.0006262625 |\n",
      "|    std                  | 1.19          |\n",
      "|    value_loss           | 1.61          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 3328         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.09744616   |\n",
      "|    clip_fraction        | 0.529        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -630         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -6.36        |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0457      |\n",
      "|    reward               | -0.024944337 |\n",
      "|    std                  | 1.2          |\n",
      "|    value_loss           | 0.187        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 3467         |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.1368731    |\n",
      "|    clip_fraction        | 0.6          |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -632         |\n",
      "|    explained_variance   | 0.532        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -5.47        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0397      |\n",
      "|    reward               | -0.079273954 |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 2.09         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = DRLAgent(env = env_train)\n",
    "model_ppo = agent.get_model('ppo')\n",
    "\n",
    "tmp_path = RESULTS_DIR + '/ppo'\n",
    "new_logger_ppo = configure(tmp_path, ['stdout', 'csv', 'tensorboard'])\n",
    "\n",
    "model_ppo.set_logger(new_logger_ppo)\n",
    "\n",
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                                tb_log_name='ppo',\n",
    "                                total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b99e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + '/agent_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drebi\\miniconda3\\envs\\summerresearch\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = PPO.load('trained_models/agent_ppo.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99462d",
   "metadata": {},
   "source": [
    "Here we are training data for the Decision Transformer (DT) model by running pre-trained PPO (Proximal Policy Optimization) reinformcement learning model through a stock trading enviroment.\n",
    "\n",
    "### Main Loop (Lines 16-37)\n",
    "\n",
    "The loop runs through the trading environment step-by-step. This creates offline trajectories with\n",
    "\n",
    "* **States**: Market observations (291-dimensional state spaces)\n",
    "* **Actions**: PPO's trading decisions (29-dimensional action space)\n",
    "* **Rewards**: Trading performance\n",
    "* **Dones**: Episode terminiation flags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7546f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 3144, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "day: 3144, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "day: 3144, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "day: 3144, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "day: 3144, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "day: 3144, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "day: 3144, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "day: 3144, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "day: 3144, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "day: 3144, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "day: 3144, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "day: 3144, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "day: 3144, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "day: 3144, episode: 150\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "day: 3144, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "day: 3144, episode: 170\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "day: 3144, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "day: 3144, episode: 190\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "day: 3144, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "day: 3144, episode: 210\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "day: 3144, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "day: 3144, episode: 230\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "day: 3144, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "day: 3144, episode: 250\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "day: 3144, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "day: 3144, episode: 270\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "day: 3144, episode: 280\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "day: 3144, episode: 290\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "day: 3144, episode: 300\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "day: 3144, episode: 310\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "day: 3144, episode: 320\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "day: 3144, episode: 330\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "day: 3144, episode: 340\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "day: 3144, episode: 350\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "day: 3144, episode: 360\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "day: 3144, episode: 370\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "day: 3144, episode: 380\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "day: 3144, episode: 390\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "day: 3144, episode: 400\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3988310.24\n",
      "total_reward: 2988310.24\n",
      "total_cost: 9075.26\n",
      "total_trades: 600746\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "1200000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"make a prediction and get results\"\"\"\n",
    "env_train, obs = e_train_gym.get_sb_env()\n",
    "\n",
    "ds = []\n",
    "states = []\n",
    "feature = {}\n",
    "\n",
    "s, a, r, d = [], [], [], []\n",
    "\n",
    "env_train.reset()\n",
    "# max_steps = len(e_train_gym.df.index.unique()) - 1\n",
    "max_steps = e_train_gym.df.tic.count() - 1\n",
    "max_steps = 100000\n",
    "\n",
    "for i in range(1, max_steps, 1):\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    s.extend(obs)\n",
    "    a.extend(action)\n",
    "\n",
    "    obs, rewards, dones, info = env_train.step(action)\n",
    "    r.extend(rewards)\n",
    "    d.append(dones[0])\n",
    "\n",
    "    states.extend(obs)\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        \n",
    "        feature['observations'] = s\n",
    "        feature['actions'] = a\n",
    "        feature['rewards'] = r\n",
    "        feature['dones'] = d\n",
    "        \n",
    "        ds.append(feature)\n",
    "        feature = {}\n",
    "        s, a, r, d = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1ffc9",
   "metadata": {},
   "source": [
    "### State Staistics\n",
    "\n",
    "This calculates the mean and standard deviation of all collected states, whcih will be used for normalization in the Decision Transformer training.\n",
    "\n",
    "The purpose is to prepare data for limitation learning. It's collecting expert demonstrations from a trained RL agent (PPO) to train a Deceision Transformer mdoel. The DT will learn to replicate the PPO agent's behavior by observing the state-action-reward sequences.\n",
    "\n",
    "The data structure `ds` contains batches of experiened tuples (observations, actions, rewards, dones) that will be used to train the Decision Transformer to make similiar trading decisions as the PPO model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a886063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.vstack(states)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76e3e420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4067.2368  ,   56.01334 ,   25.70047 ,  112.767555,   44.781887],\n",
       "       dtype=float32),\n",
       " array([5.2179863e+04, 3.6167255e+01, 1.4407150e+01, 4.4108833e+01,\n",
       "        4.3545811e+01], dtype=float32),\n",
       " (3941,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mean[:5], state_std[:5], state_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dfa25ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3941"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6272bed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3bc39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ds = len(ds)\n",
    "\n",
    "state_mean = np.pad(state_mean, (0, (len_ds-state_space)))\n",
    "state_std = np.pad(state_std, (0, (len_ds-state_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8e6e235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4067.2368 ,   56.01334,   25.70047, ...,    0.     ,    0.     ,\n",
       "           0.     ], shape=(4999,), dtype=float32),\n",
       " 4999)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mean, len(state_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f67422d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds), len(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7893acaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = ds[0]\n",
    "len(feature['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e89032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {}\n",
    "input_data['train'] = ds\n",
    "input_data['state_mean'] = state_mean\n",
    "input_data['state_std'] = state_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ea08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'state_mean', 'state_std'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3dfa13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"input_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(input_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4c3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"input_data.pkl\", \"rb\") as f:\n",
    "    input_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56db564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59ad0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0373de9c6d344b989628a27c4802b4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/18 shards):   0%|          | 0/4999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"data/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e28154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb2a0cab49449e0af7f30699068ad94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"data/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4a728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summerresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
