{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd10e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2209a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5115fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fab9cf955ad4cae82932d70b3d81c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_from_disk('data/dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3cae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mean = dataset['state_mean']\n",
    "state_std = dataset['state_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a301748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55aee951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5189ba24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['actions', 'dones', 'observations', 'rewards'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "537f183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_dim = len(dataset[0]['actions'][0])\n",
    "act_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f303fcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3941"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = len(dataset[0]['observations'][0])\n",
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "956e83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mean = state_mean[:state_dim]\n",
    "state_std = state_std[:state_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "276a2c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0]['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bce9836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.0671696737408638,\n",
       "  -0.18361186981201172,\n",
       "  0.2469455599784851,\n",
       "  -0.01885118894279003,\n",
       "  0.10628541558980942,\n",
       "  -0.2710021436214447,\n",
       "  0.07851219177246094,\n",
       "  0.23499460518360138,\n",
       "  0.25581544637680054,\n",
       "  0.05722948908805847,\n",
       "  0.2383994609117508,\n",
       "  -0.11575343459844589,\n",
       "  0.18378931283950806,\n",
       "  -0.20906339585781097,\n",
       "  -0.0037708738818764687,\n",
       "  0.1894841343164444,\n",
       "  0.20058195292949677,\n",
       "  -0.16733631491661072,\n",
       "  -0.20673605799674988,\n",
       "  -0.04827669635415077,\n",
       "  -0.02904391475021839,\n",
       "  0.1855313777923584,\n",
       "  -0.393166184425354,\n",
       "  -0.083065465092659,\n",
       "  -0.09834568947553635,\n",
       "  -0.136605367064476,\n",
       "  -0.5146928429603577,\n",
       "  0.1899474561214447,\n",
       "  0.007351686712354422],\n",
       " [-0.0671696737408638,\n",
       "  -0.18361186981201172,\n",
       "  0.2469455599784851,\n",
       "  -0.01885118894279003,\n",
       "  0.10628541558980942,\n",
       "  -0.2710021436214447,\n",
       "  0.07851219177246094,\n",
       "  0.23499460518360138,\n",
       "  0.25581544637680054,\n",
       "  0.05722948908805847,\n",
       "  0.2383994609117508,\n",
       "  -0.11575343459844589,\n",
       "  0.18378931283950806,\n",
       "  -0.20906339585781097,\n",
       "  -0.0037708738818764687,\n",
       "  0.1894841343164444,\n",
       "  0.20058195292949677,\n",
       "  -0.16733631491661072,\n",
       "  -0.20673605799674988,\n",
       "  -0.04827669635415077,\n",
       "  -0.02904391475021839,\n",
       "  0.1855313777923584,\n",
       "  -0.393166184425354,\n",
       "  -0.083065465092659,\n",
       "  -0.09834568947553635,\n",
       "  -0.136605367064476,\n",
       "  -0.5146928429603577,\n",
       "  0.1899474561214447,\n",
       "  0.007351686712354422]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['actions'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e9ce4",
   "metadata": {},
   "source": [
    "### Class Overview\n",
    "\n",
    "This is a data collator that prepares batches of RL trajectories for training a Decision Transformer model. It handles the complex task of sampling trajectory segments, computing returns-to-go, and formatting everything for the transformer architecture.\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "1. **Random segment sampling**: Instead of using full episodes, samples random segments for better generalzation.\n",
    "2. **Undiscounted returns**: Use `gamma=1.0` rather than traditional RL discounting.\n",
    "3. **Sentinel padding**: Uses distinct sentinel values $(-10.0, 2)$ to distinguish padding from real data.\n",
    "4. **State normalization**: Normalizes states using dataset statistics for stable training.\n",
    "5. **Length-weighted sampling**: Intended to give more weight to longer trajectories.\n",
    "\n",
    "### Purpose in Decision Transformer Training\n",
    "\n",
    "This collator enables the Decision Transformer to learn from offline RL data by:\n",
    "\n",
    "* Conditioning actions on states, returns-to-go, and timesteps\n",
    "* Using attention mechanism to process variable-length sequence\n",
    "* Learning to predict actions given desired future returns\n",
    "\n",
    "The resulting batch format matches exactly what the Decision Transformer model expects for training.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "1. **Dynamic dimension detection (lines 15 - 16)**: Automatically detects the action dimention `act_dim` and the state dimension `state_dim` from the first sample.\n",
    "2. **Store normalization stats (line 18-19)**: Keeps the state mean `state_mean` and state standard deviation `state_std` for state normalization.\n",
    "3. **Trajectory sampling weights (lines 24-28)**: \n",
    "\n",
    "### Return-to-Go Calculation\n",
    "\n",
    "**Lines 30-35**: Computes discounted comulative returns:\n",
    "\n",
    "* Takes rewards and gamma (discount factor)\n",
    "* Computes backwards cumulative sum \n",
    "* Used the Decision Transformer's condition on future returns\n",
    "\n",
    "### Main Batch Processing\n",
    "\n",
    "#### 1. Batch Sampling (lines 38-45)\n",
    "\n",
    "This part samples trajectory indices according to `p_sample` distribution.\n",
    "\n",
    "#### 2. Sequence Extraction (lines 47-69)\n",
    "\n",
    "For each sample trajectory\n",
    "\n",
    "* **Random start point (line 52)**:\n",
    "* **Extract sequences (lines 55-57)**: Takes the maximum length `max_len` timesteps starting from `si`.\n",
    "* Compute returns-to-go (lines 62-66): \n",
    " - uses `_discount_cumsum` with `gamma=1.0` (undiscounted)\n",
    " - Takes only the first `max_len elements to match sequence length\n",
    "* **Handling edge case (lines 67-69): If RTG is shorter than sequence, pad with zeros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd305338",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 3941  # size of state space\n",
    "    act_dim: int =  394 # size of action space\n",
    "    max_ep_len: int = 985 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    #state_mean: np.array = None  # to store state means\n",
    "    #state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset, state_mean, state_std) -> None:\n",
    "        self.act_dim = len(dataset[0]['actions'][0])\n",
    "        self.state_dim = len(dataset[0]['observations'][0])\n",
    "        self.dataset = dataset\n",
    "        self.state_mean = state_mean\n",
    "        self.state_std = state_std\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "\n",
    "        self.n_traj = len(self.dataset)\n",
    "\n",
    "        traj_lens = [len(self.dataset[i]) for i in range(self.n_traj)]\n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "\n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25893b6",
   "metadata": {},
   "source": [
    "Here we are creating an instance of the data collator called `collator` that will be used by the Hugging Face `Trainer` to prepare batches of training data for the Decision Transformer model.\n",
    "\n",
    "This uses the `dataset` which are the PPO-generated trajectory data loaded from disk, the state normalization statistics: the mean values  (`state_mean`) and the standard deviation (`state_std`).\n",
    "\n",
    "Then the collator will do the following:\n",
    "\n",
    "* Sample random trajectory segments during training\n",
    "* Compute returns-to-go for each sequence\n",
    "* Pad sequence to uniform length\n",
    "* Normalize states using the provided statistics\n",
    "* Convert everything to PyTorch tensors\n",
    "\n",
    "The Decision Transformers have specific data requirements that differ from standard transformer models\n",
    "\n",
    "* **Sequence formatting**: Need states, actions, rewards, returns-to-go, and timesteps\n",
    "* **Attention masking**: Requires proper masking for variable-length sequences\n",
    "* **Random sampling**: Samples random trajectory segments rather than full episodes\n",
    "\n",
    "The `Trainer` class doesn't know how to handle RL trajectory data, so the custom collator bridges this gap by converting the raw dataset into the exact format the Decision Transformer expects the training.\n",
    "\n",
    "In essence, this line prepares the data processing pipeline the will tranform raw PPO trajectories into properly formatting training batches for the Decision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "402b0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset, state_mean, state_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1381fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6090c74",
   "metadata": {},
   "source": [
    "The **main purpose** is to bridge the gap between:\n",
    "\n",
    "* **Decision Transformer**: Which normally returns model outputs (logits, hidden states, etc.)\n",
    "* **Hugging Face Trainer**: Which expects models to return a loss dictionary during training\n",
    "\n",
    "\n",
    "This is the key modification that makes the model trainable with Hugging Face's `Trainer`:\n",
    "\n",
    "1. **Get base model output**: `output = super().forward(**kwarg)` calls the original Decision Transformer forward pass\n",
    "2. **Extract action prediction**: `action_preds = output[1]` get the predicted actions (the second element of the output tuple)\n",
    "3. **Get target actions**: `action_targets = kwargs['actions']` gets the ground truth actions from the batch\n",
    "4. **Get attention mask**: `attention_mask = kwargs['attention_mask']` gets the mask that identifies real vs. padding tokens\n",
    "5. **Flatten and filter**:\n",
    "* Reshapes predictions and target to 2D: `(batch_size * seq_len, action_dim)`\n",
    "* Uses the attention mask to select only real (non-padding) token: `[attention_mask.reshape(-1) > 0]`\n",
    "6. **Compute MSE Loss**: `torch.mean((action_preds - action_targets) ** 2)` calculates mean squared error between predicted and target actions\n",
    "7. **Return loss dict**: Returns `{'loss':loss}` which is the format expected by Hugging Face `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c8d4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "\n",
    "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d627b0",
   "metadata": {},
   "source": [
    "This creates a configuration object for the Decision Transformer with:\n",
    "\n",
    "* `state_dim=collator.state_dim`: Sets the state space dimension (291 dimensions)\n",
    "* `act_dim=collator.act_dim`: Sets the action space dimension (29, one action per stock)\n",
    "\n",
    "The configuration object contains all the hyperparameters and architectural settings needed to build the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5723c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e12c0",
   "metadata": {},
   "source": [
    "This creates an instance of the custom `TrainsableDT` class using the configuration:\n",
    "* Instantiates the transformer architecture with the specified dimensions\n",
    "* Sets up all the neural networks layers (embeddings, attention blocks, output heads)\n",
    "* Makes it compatible with Hugging Face Trainer (due to the custom `TrainerDT` wrapper)\n",
    "\n",
    "The `DecisionTransformerConfig` automatically sets up the Token embeddings, transformer encoder with attention mechanisms, output heads for predicting actions, and the default hyperparameters (hidden size, number of layers, etc.)\n",
    "\n",
    "The creates the trainable model that will:\n",
    "1. Recieve bateches from the data collator (states, actions, rewards, etc.)\n",
    "2. Process sequences through the transformer architecture\n",
    "3. Predict actions conditioned on states and returns-to-go\n",
    "4. Compute loss against ground truth actions (via the custom `forward` method)\n",
    "5. Update weights through backpropagation during training\n",
    "\n",
    "This is the core model that will learn to replication the PPO agent's trading beahvior in a conditional, goal-directed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff45717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TrainableDT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9aad5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9865f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=120,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a50348",
   "metadata": {},
   "source": [
    "### Parameter Breakdown\n",
    "\n",
    "#### Training Duration\n",
    "* `num_train_epochs=120`: Train for 120 complete epochs (full passes through the dataset)\n",
    "* `per_device_train_batch_size=64`: Process 64 trajectory segments per batch\n",
    "\n",
    "#### Optimization\n",
    "* `learning_rate=1e-4`: Learning rate of 0.00001 (conservative for transformer training)\n",
    "* `weight_decay=1e-4`: L2 regularization to prevent overfitting\n",
    "* `optim='adamw_torch`: Uses AdamW optimizer (Adam with decoupled weight decay)\n",
    "* `max_grad_norm=0.25`: Clips gradients to prevent exploding gradients\n",
    "\n",
    "#### Learning Schedule \n",
    "* `warmup_ratio=0.1`: Gradually increases learning rate from 0 to full rate over first 10% of training (12 epochs)\n",
    "\n",
    "#### Data Handling\n",
    "* `remove_unused_columns=False`: Keeps all dataset columns (important for RL data which has states, actions, rewards, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "150dbc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "194741e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9480' max='9480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9480/9480 39:37, Epoch 120/120]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9480, training_loss=0.001196348162187177, metrics={'train_runtime': 2378.5035, 'train_samples_per_second': 252.209, 'train_steps_per_second': 3.986, 'total_flos': 4.851370333119744e+17, 'train_loss': 0.001196348162187177, 'epoch': 120.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26aad8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c3858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summerresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
